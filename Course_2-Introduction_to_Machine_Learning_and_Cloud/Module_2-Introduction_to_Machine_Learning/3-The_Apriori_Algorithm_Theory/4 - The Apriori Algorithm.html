<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>4 - The Apriori Algorithm</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="the-apriori-algorithm">The Apriori Algorithm</h1>
<p>In this segment, you will have a quick walk-through of the steps involved in building the algorithm. Consider a store transaction database that consists of all the transactions made by each customer. Let’s assume that there are only four items in the store {A, B, C, D} and each transaction consists of at least one of the four items. A visual representation of all the possible transaction is as shown below.</p>
<p><img src="https://i.ibb.co/J7QjCDp/Apriori-Algorithm.jpg" alt="Apriori_Algorithm"></p>
<p>To generate all the frequent itemsets from a list of transactions you use the Apriori principle.</p>
<p><strong>The Apriori Principle:</strong></p>
<ul>
<li>
<p>If an itemset is found to be frequent (support value is greater than the minimum threshold), then all of its lower order itemsets must be frequent.</p>
<p>For example, if the itemset {bread, butter, milk} is found to be frequent, then all of its lower order itemsets will be frequent. Thus, the itemsets {bread}, {butter}, {milk}, {bread, butter}, {bread, milk} and {butter, milk} will be frequent.</p>
</li>
<li>
<p>Similarly, if an itemset is found to be infrequent, all of its consequent higher-order itemsets will be infrequent.<br>
For example, if the itemset {bread, orange} is found to be infrequent (support value is less than the support threshold), then all itemsets containing three or more items having both bread and orange will automatically become infrequent.</p>
<p><img src="https://i.ibb.co/fNXyC1W/Apriori-Algorithm-Explanation.jpg" alt="Apriori_Algorithm_Explanation"></p>
</li>
</ul>
<p>In the next video, let’s understand the two major steps involved in building the algorithm.</p>
<p><strong>VIDEO</strong></p>
<p>As discussed, we have divided this implementation into the following two parts:</p>
<ul>
<li>
<p>Frequent Itemset Generation</p>
</li>
<li>
<p>Rule Generation</p>
</li>
</ul>
<p><strong>Frequent Itemset Generation in Apriori Algorithm:</strong></p>
<ul>
<li>
<p>The first step of the algorithm is to  <strong>identify distinct items</strong>  in the given set of transactions. Let’s say these are ({A}, {B}, {C}, {D}).</p>
</li>
<li>
<p>Once you have different items, your next step would be to  <strong>calculate the support of each of these items</strong>. Items with support values less than the minimum support are removed from the distinct items list.</p>
</li>
<li>
<p>The next step is to  <strong>create higher-order itemsets</strong>  by merging the existing itemsets. This can be done  <strong>using the candidate generation technique</strong>. We will cover this concept in detail while implementing the generation of higher item sets on code.</p>
<p>Using the 1 itemsets ({A}, {B}, {C}, {D}) and assuming that only A, B and C are frequent, we generate itemsets {A, B}, {A, C} and {B, C}. Note that none of the 2-item sets contains the item D. This is because we have applied the  <strong>Apriori principle</strong>. Since D is infrequent, any item set containing D, e.g., {A, D}, {C, D} and {B, C, D} will automatically become infrequent.</p>
</li>
<li>
<p>Once you have the  <strong>higher-order itemsets</strong>, you can  <strong>calculate the support</strong>  for these item sets and again  <strong>remove the itemsets that do not qualify the minimum support criteria</strong>.</p>
</li>
<li>
<p>This ( n-1 ) -item sets then become inputs for the generation of n-item sets, and once again the item sets that do not satisfy the minimum support criteria are removed. This  <strong>process continues until no new itemsets can be generated</strong>.</p>
</li>
</ul>
<p><strong>Rule Generation in the Apriori Algorithm:</strong></p>
<ul>
<li>
<p>Once you have all the frequent itemsets, we can proceed with the rule generation process. We begin with 2-itemsets and generate all the possible rules.</p>
</li>
<li>
<p>For each rule, we check the corresponding confidence value and return the rule only if its  <strong>confidence is above the minimum confidence level</strong>.</p>
</li>
<li>
<p>In order to  <strong>avoid generating redundant rules, we utilize confidence-based pruning</strong>. Using this, we  <strong>eliminate the generation of higher-order rules</strong>  if their corresponding lower-order rules are infrequent. This portion will be explained in more detail in the code demonstration of rule generation.</p>
</li>
</ul>
<p>In the next session, we will begin the implementation of the algorithm in Python. A summary of all the components associated with Apriori and the lecture notes have been provided in the next session.</p>
<h4 id="support-and-confidence">Support and Confidence</h4>
<p>Qn: Which of the following is the most interesting association rule?</p>
<ul>
<li>High support and high confidence</li>
<li>High support and low confidence</li>
<li>Low support and high confidence</li>
<li>Low support and low confidence</li>
</ul>
<p>Ans: Low support and high confidence - Such rules are most interesting since they are usually unexpected. An example of such a rule will be {Apples} —&gt; {Diapers}.</p>
<p>Note: The support must be lower compared to the support of the other frequent items but must still be above the minimum support threshold so that we can classify the item set as frequent. The objective of this algorithm is not to extract rules with higher support and higher confidence but to extract the interesting rules that suggest some patterns. This question has only been given for your general knowledge. In the code implementation, Sajan has printed all the frequent item sets and the valid rules which satisfy the minimum support and minimum confidence threshold respectively.</p>
<h4 id="association-rules">Association rules</h4>
<p>Qn: Which of the following metrics governs the interestingness for association rules? (More than one option might be correct). Additional Read -  <a href="https://en.wikipedia.org/wiki/Lift_(data_mining)#:~:text=In%20data%20mining%20and%20association,a%20random%20choice%20targeting%20model.">Lift</a></p>
<ul>
<li>Support</li>
<li>Lift</li>
<li>Accuracy</li>
<li>Confidence</li>
</ul>
<p>Ans: Lift &amp; Confidence</p>
<ul>
<li>Support is a key metric used for governing frequent itemsets, not rules.</li>
<li>In the previous question, you already saw that even if both support and confidence are high, the rule may not be very useful. Sometimes, we calculate the lift value since it puts more emphasis on the rules having higher confidence and lower support. In other words, the lift is calculated to judge the quality of a rule.<br>
For the given rule {A} → {B}, lift is mathematically defined as follows:<br>
*<em>Lift = support(A, B)/(support(A)<em>support(B))</em></em><br>
or<br>
<strong>Lift = confidence(rule)/ support (rule consequent)</strong><br>
Note that this concept has only been covered for your knowledge and has not been covered in the code demonstration.</li>
<li>Confidence is one of the most frequently used metrics for extracting the association rules.</li>
</ul>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
</div>
</body>

</html>
