<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>10 - Unsupervised Learning Algorithms: II</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="unsupervised-learning-algorithms-ii">Unsupervised Learning Algorithms: II</h1>
<p>Let’s look at another unsupervised learning technique -  <strong>association rule mining.</strong></p>
<p><strong>VIDEO</strong></p>
<p>Association rule mining is an unsupervised learning technique used to identify interesting relationships between variables in large databases.</p>
<p>One important algorithm under the class of association rule mining algorithms is the  <strong>apriori algorithm</strong>. It was developed to process transactional data and identify associations in the data. This algorithm, when applied to data collected from certain Walmart stores, led to the identification of an interesting revelation, which was later termed <strong>“Walmart’s beer diaper parable”</strong>. According to the analysis, there is a strong association between beer and diaper sales, which made no sense initially.</p>
<p>In the video, you also learnt about a few rule generation metrics</p>
<ol>
<li><strong>Support</strong> denotes the frequency of occurrence of an item
<ul>
<li>Used to form itemsets</li>
</ul>
</li>
<li><strong>Confidence</strong>  denotes the likelihood of occurrence of an event given another event has occurred.
<ul>
<li>Used to determine the validity of a rule</li>
</ul>
</li>
</ol>
<p>In the next video, let’s understand the working of the apriori algorithm by considering a transaction database.</p>
<p><strong>VIDEO</strong></p>
<p>Let’s revisit the steps involved in the algorithm:</p>
<ul>
<li>
<p>Frequent Itemset Generation</p>
</li>
<li>
<p>Rule Generation</p>
</li>
</ul>
<p><strong>Frequent Itemset Generation</strong>  involves generating all possible k-order itemsets and calculating the support of each itemset. Here, the itemsets having support values greater than the minimum Support are termed as frequent itemsets and are used to generate higher-order itemsets. The next step would be  <strong>rule generation</strong>  where you consider all the frequent itemsets to generate rules which have a confidence value more than the defined threshold value. You will learn more about these steps in the future sessions, where you will be building the apriori algorithm from scratch.</p>
<p>Although this algorithm is easy to understand and implement, it has its drawbacks. It is a very computationally expensive algorithm. To compute the support for each itemset, you have to go through the entire database of transactions and check whether the itemset is a part of the transaction or not. Applying such an algorithm on a smaller dataset involving 20 items and a hundred purchases wouldn’t be a challenging task, but let’s say you are dealing with the sales happening on Amazon or Flipkart, You can no longer handle such jobs with regular infrastructure. You need the cloud to store the massive amount of data and have multiple instances in the cloud to run the algorithm.</p>
<p>We need to create the required number of instances in the cloud and use distributed filesystem frameworks such as Hadoop File System (HDFS) to store the billion transaction inputs to the algorithm in multiple nodes. We use frameworks such as MapReduce to write our distributed programs to run on these datasets, determine frequently occurring itemsets in the database and use this to discover the association rules present.</p>
<p><strong>Apriori Algorithm: Comprehension</strong></p>
<p>Consider the following database to identify association rules:</p>
<p><img src="https://i.ibb.co/dcm9X5r/Database-Association-Rules.png" alt="Database-Association-Rules"></p>
<p>Assume that the threshold values are defined as follows:</p>
<ul>
<li>Support = 0.5</li>
<li>Confidence = 0.8</li>
</ul>
<p>Based on this information, answer the following question:</p>
<h4 id="apriori-algorithm">Apriori Algorithm</h4>
<p>Qn: What is the support value for oranges?<br>
Ans: Support denotes the frequency of occurrence of an item. It is calculated as the ratio of the number of transactions in which the item has appeared divided by the total number of transactions.</p>
<p>Support(orange) = 8/10 = 0.8</p>
<p>Qn: What is the final set of one-item sets which qualify against the minimum threshold?</p>
<ul>
<li>{{apple}, {orange}, {banana}, {kiwi}}</li>
<li>{{apple, orange, banana}}</li>
<li>{{apple, orange}, {kiwi}}</li>
<li>{{apple}, {orange}, {banana}}</li>
</ul>
<p>Ans: Calculate the support values of all the items involved, and items with support values greater than 0.5 will be part of the one-item set.</p>
<ul>
<li>support(apple) = 0.8</li>
<li>support(orange)= 0.8</li>
<li>support(kiwi)= 0.4</li>
<li>support(banana)= 0.7</li>
<li>support(kiwi) = 0.4</li>
</ul>
<p>Since support values of apple, orange and banana are greater than 0.5 they will be part of the final set of 1 itemset.</p>
<p>Qn: What is the final set of 3-item sets which qualify against the minimum threshold?</p>
<ul>
<li>{{apple}, {orange}, {banana}, {kiwi}}</li>
<li>{{apple, orange, banana}}</li>
<li>No such set exists</li>
<li>{{apple}, {orange}, {banana}}</li>
</ul>
<p>Ans: In the previous question, you found that only the items apple, orange and banana qualify against the minimum threshold value. So any three-item sets containing items other than these three wouldn’t qualify against the minimum threshold. If you check the support value for the itemset {orange, apple, banana}, it comes out to be 0.4. This means that there is no three-item set which qualifies against the minimum support criteria.</p>
<p>Qn: What is the confidence of the rule {apple} =&gt; {banana}? [round up to one decimal value]?<br>
Ans: Confidence denotes the likelihood of occurrence of an event. The confidence of the rule {apple} =&gt; {banana} is</p>
<p>support({apple, banana})/support({apple}) = 0.5/0.8 = 0.6</p>
<p>Qn: Identify the set of final rules generated.</p>
<ul>
<li>{apple} =&gt; {orange}, {apple} =&gt; {banana}, {orange} =&gt; {apple}, {banana} =&gt; {apple}</li>
<li>{orange} =&gt; {apple}, {apple} =&gt; {orange}</li>
<li>{orange} =&gt; {banana}, {apple} =&gt; {banana}</li>
<li>{apple} =&gt; {orange}, {orange} =&gt; {banana}, {orange} =&gt; {apple}, {banana} =&gt; {orange}</li>
</ul>
<p>Ans: Confidence of a rule {A} =&gt; {B} is support(A, B)/support(A).<br>
The support values are as follows:<br>
{apple} = 0.8, {orange} = 0.8, {banana} = 0.7<br>
{apple, orange} = 0.7<br>
{orange, banana} = 0.5<br>
{apple, banana} = 0.5</p>
<p>If you calculate the confidence values of all the possible rules, only the confidence values of the rules {orange} =&gt; {apple} and<br>
{apple} =&gt; {orange} are above the threshold 0.8.</p>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
</div>
</body>

</html>
