{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 263 µs (started: 2022-08-17 05:54:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Loading autotime for the notebook\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.06 ms (started: 2022-08-17 05:54:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook --no-browser\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 707 µs (started: 2022-08-17 08:41:03 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Spark environment\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Python Utilites\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecommerce Churn Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the assignment is to build a model that predicts whether a person purchases an item after it has been added to the cart or not. Being a classification problem, you are expected to use your understanding of all the three models covered till now. You must select the most robust model and provide a solution that predicts the churn in the most suitable manner. \n",
    "\n",
    "For this assignment, you are provided the data associated with an e-commerce company for the month of October 2019. Your task is to first analyse the data, and then perform multiple steps towards the model building process.\n",
    "\n",
    "The broad tasks are:\n",
    "- Data Exploration\n",
    "- Feature Engineering\n",
    "- Model Selection\n",
    "- Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset stores the information of a customer session on the e-commerce platform. It records the activity and the associated parameters with it.\n",
    "\n",
    "- **event_time**: Date and time when user accesses the platform\n",
    "- **event_type**: Action performed by the customer\n",
    "            - View\n",
    "            - Cart\n",
    "            - Purchase\n",
    "            - Remove from cart\n",
    "- **product_id**: Unique number to identify the product in the event\n",
    "- **category_id**: Unique number to identify the category of the product\n",
    "- **category_code**: Stores primary and secondary categories of the product\n",
    "- **brand**: Brand associated with the product\n",
    "- **price**: Price of the product\n",
    "- **user_id**: Unique ID for a customer\n",
    "- **user_session**: Session ID for a user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provided is 5 GBs in size. Therefore, it is expected that you increase the driver memory to a greater number. You can refer to notebook 1 for the steps involved here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-15-133.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0ac755b210>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.94 s (started: 2022-08-17 05:54:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = \"14G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"demo\") \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 285 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Loading the clean data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Selection\n",
    "3 models for classification:\t\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 837 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Additional steps for Decision Trees, if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformation (Code will be same; check for the columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 490 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Check if only the required columns are present to build the model\n",
    "# If not, drop the redundant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 432 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Categorising the attributes into its type - Continuous and Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 424 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Feature transformation for categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 423 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Vector assembler to combine all the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 368 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for the tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 911 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Transforming the dataframe df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 417 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Schema of the transformed df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 406 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Checking the elements of the transformed df - Top 20 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 417 µs (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Storing the transformed df in S3 bucket to prevent repetition of steps again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.83 s (started: 2022-08-17 05:54:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Load transformed data\n",
    "df_transformed = spark.read.parquet(\"Parquets/transformed_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 69.8 ms (started: 2022-08-17 05:54:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into train and test (Remember you are expected to compare the model later)\n",
    "df_train, df_test = df_transformed.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train rows: 628038\n",
      "Number of Test rows: 270405\n",
      "time: 22.8 s (started: 2022-08-17 05:54:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in train and test data\n",
    "print(f\"Number of Train rows: {df_train.count()}\")\n",
    "print(f\"Number of Test rows: {df_test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 395 µs (started: 2022-08-17 05:54:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "label_column = \"is_purchased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 26s (started: 2022-08-17 05:54:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=label_column, \n",
    "                                       featuresCol=\"features\", \n",
    "                                       numTrees=10)\n",
    "    \n",
    "# Chain indexer and dtc together into a single ML Pipeline.\n",
    "model = random_forest.fit(df_train)\n",
    "\n",
    "# Define an evaluation metric and evaluate the model on the validation dataset.\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_column, metricName=\"accuracy\")\n",
    "predictions = model.transform(df_test)\n",
    "validation_metric = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for basic RF model: 0.636460124627873\n",
      "time: 793 µs (started: 2022-08-17 05:56:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for basic RF model: {validation_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2h 26min 8s (started: 2022-08-17 05:56:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Building the model with hyperparameter tuning\n",
    "# Create ParamGrid for Cross Validation\n",
    "\n",
    "# Initialising RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(labelCol=label_column, \n",
    "                                       featuresCol=\"features\", \n",
    "                                       featureSubsetStrategy=\"auto\",\n",
    "                                       seed=42)\n",
    "\n",
    "# Creating Parameter Grid search on RF model\n",
    "num_trees = [10, 20, 30]\n",
    "max_depth= [3, 5, 7, 9]\n",
    "max_bins= [16, 32, 64]\n",
    "impurity = [\"gini\", \"entropy\"]\n",
    "\n",
    "param_grid = ParamGridBuilder().addGrid(random_forest.numTrees, num_trees) \\\n",
    "                               .addGrid(random_forest.maxDepth, max_depth) \\\n",
    "                               .addGrid(random_forest.maxBins, max_bins) \\\n",
    "                               .addGrid(random_forest.impurity, impurity) \\\n",
    "                               .build()\n",
    "\n",
    "class_evaluator = MulticlassClassificationEvaluator(labelCol=label_column, \n",
    "                                                    metricName=\"accuracy\")\n",
    "\n",
    "cross_validator = CrossValidator(estimator=random_forest,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=class_evaluator,\n",
    "                                 numFolds=10,\n",
    "                                 parallelism=4)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cross_validator_model = cross_validator.fit(df_train)\n",
    "\n",
    "# Make predictions on testing data and calculating ROC metrics and model accuracy. \n",
    "prediction = cross_validator_model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 449 µs (started: 2022-08-17 08:22:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 432 µs (started: 2022-08-17 08:22:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Fitting the models on transformed df\n",
    "# Run cross-validation, and choose the best set of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 260 µs (started: 2022-08-17 08:22:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Best model from the results of cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Steps:\n",
    "- Fit on test data\n",
    "- Performance analysis\n",
    "    - Appropriate Metric with reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the best Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 30\n",
      "maxDepth = 9\n",
      "maxBins = 64\n",
      "impurity = entropy\n",
      "time: 1.03 ms (started: 2022-08-17 08:41:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "best_model_params = cross_validator_model.getEstimatorParamMaps()[np.argmax(cross_validator_model.avgMetrics)]\n",
    "param_keys = list(best_model_params.keys())\n",
    "for param in param_keys:\n",
    "    print(f\"{param.name} = {best_model_params[param]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.4 ms (started: 2022-08-17 08:49:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "best_model = cross_validator_model.bestModel\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_column, metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22 s (started: 2022-08-17 08:50:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "best_model_result = evaluator.evaluate(best_model.transform(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6491411031600747\n",
      "time: 598 µs (started: 2022-08-17 09:03:44 +00:00)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {best_model_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 s (started: 2022-08-17 09:03:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "rf_model_path = \"Models/RandomForest\"\n",
    "best_model.save(rf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
