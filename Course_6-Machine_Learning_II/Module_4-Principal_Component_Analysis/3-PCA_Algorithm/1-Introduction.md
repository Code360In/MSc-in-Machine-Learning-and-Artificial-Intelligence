# Introduction

Welcome to the third session of the module on ‘Principal Component Analysis (PCA)’. In the previous session, you saw Python demonstrations for vectors and matrices. You also learnt that the relationship between variables can be depicted in the form of a covariance matrix. You have also been through the important building block of PCA: Basis, and learnt how basis can be changed.

Now, in this session, you will learn which ‘change of basis matrix’ should be used in order to convert all the original data points to new data points in a new basis system, in such a way that the covariance between the new variables (also known as Principal Components) is negligible.

So, our ultimate objective is to get the appropriate ‘change of basis matrix’, such that you get the new data points whose covariance matrix is a diagonalised matrix, which means the covariance between the variables becomes negligible.

So, in this session, we will broadly cover the following topics.

1.  Eigenvectors and Eigenvalues
2.  Diagonalisation of a Covariance Matrix
3.  Eigendecomposition of a Covariance Matrix
4.  PCA Algorithm
5.  Python Demonstration of PCA Algorithm
6.  Spark MLlib Demonstration of PCA 

## People you will hear from in this session

Subject Matter Experts  
[Jaidev Deshpande](https://www.linkedin.com/in/jaidevd/)  
Senior Data Scientist, Gramener

With over 10 years of experience in data science and predictive analytics, Jaidev Deshpande has worked in multiple firms, including Springboard, iDataLabs and Cube26. He received his bachelor’s degree in Electrical and Electronics Engineering from Vishwakarma Institute of Technology, Pune, and is currently working as a Senior Data Scientist at Gramener, a leading data science consulting company that advises clients on data-driven leadership.