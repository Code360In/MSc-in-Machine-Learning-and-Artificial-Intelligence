# Introduction

Welcome to the first module on ‘Unsupervised Learning’. In this module, you will learn about one of the common unsupervised techniques that is used for reducing the number of features of a particular data set without any loss of information: Principal Component Analysis (PCA).

In order to understand the inner workings of PCA, you need to first understand about its three main building blocks, which are as follows:

1.  **Summary statistics:** This includes mean, median, mode, variance, standard deviation and covariance.
2.  **Vectors**: This involves vector operations such as addition, scalar multiplication, basis and change of basis.
3.  **Matrices:** This includes representation of the data in the form of matrix and vectors, matrix operations, etc.

Out of the aforementioned three tools, summary statistics are covered in a separate optional module. You are expected to complete the ‘Summary Statistics’ module before starting this module.

As mentioned previously, PCA helps in dimensionality reduction by capturing maximum information. It does this by capturing the variance in the data. So, let’s watch the next video to develop an intuitive understanding of variance and learn how it captures maximum information from the data set.

**VIDEO**

So, in the video above, you saw that you can capture maximum information when you sit at the perpendicular bisector of all the panellists. In simple terms, you will be able to capture maximum information along a particular direction when you have maximum variation.

The following topics will be covered in this module on PCA:

1.  Introduction to PCA and Dimensionality Reduction
2.  Vector Representations of Data
3.  Vector Operations along with Python Demonstrations
4.  Matrix Representation of Vectors
5.  Matrix Operations along with Python Demonstrations
6.  Basis
7.  Change of Basis
8.  Covariance and Correlation Matrices
9.  Eigenvectors and Eigenvalues
10.  Diagonalisation of a Covariance Matrix
11.  Eigendecomposition of a Covariance Matrix
12.  PCA Algorithm
13.  Recommendation System using PCA in Spark

The following topics will be covered in this session:

1.  Introduction to Dimensionality Reduction
2.  Introduction to Principal Component Analysis (PCA)
3.  Vectors and their Operations
4.  Matrices and their Operations
5.  Covariance Matrix

## People you will hear from in this session:

Subject Matter Expert  
[Jaidev Deshpande](https://www.linkedin.com/in/jaidevd/)  
Senior Data Scientist, Gramener

With over 10 years of experience in data science and predictive analytics, Jaidev Deshpande has worked in multiple firms, including Springboard, iDataLabs and Cube26. He received his bachelor’s degree in Electrical and Electronics Engineering from Vishwakarma Institute of Technology, Pune, and is currently working as a Senior Data Scientist at Gramener, a leading data science consulting company that advises clients on data-driven leadership.

[Mirza Rahim Baig](https://www.linkedin.com/in/rahim-baig)

Analytics Lead, Flipkart

Flipkart is one of the leading e-commerce companies in India. It started with selling books and has now expanded its business to almost every product category, including consumer electronics, fashion and lifestyle products. Rahim is currently the Analytics Lead at Flipkart. He holds a graduate degree from BITS Pilani, a premier educational institute in India.