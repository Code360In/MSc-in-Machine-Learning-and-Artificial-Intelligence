# Introduction to Random Forests

Now that you have learnt about ensembles, you will gain an understanding of one of the most popular ML models, **Random Forest**, which is based on the bagging technique. Let's start by understanding the basics of the algorithm.

**VIDEO**

The random forest algorithm is an ensemble of decision trees that uses bagging to generate different base models. Random forests are by far the most successful model among the bagging ensembles. Let's try to understand the terms in bagging – bootstrapping and aggregation – in more detail.

### Random Forest: Ensemble of Decision Trees

![Random Forest Ensemble of Decision Trees](https://i.ibb.co/nQmkY5H/Random-Forest-Ensemble-of-Decision-Trees.jpg)
 
**Bootstrapping** refers to creating bootstrap samples from a given data set. A bootstrap sample is created by sampling the given data set uniformly and with replacement. This means that data points will overlap in different subsets. A bootstrap sample typically contains approximately 40%–70% data from the data set. The base models in the algorithm are generated by implementing all the steps mentioned under decision trees on each subset. Finally, **aggregation** implies combining the results of different models present in the ensemble. You must understand that bagging is a technique in itself and is not specific to random forests.

For the random forest algorithm to be a successful ensemble model, it should meet all the criteria of diversity and acceptability. Training an optimally deep decision tree independently on each subset makes the models acceptable, as a decision tree's performance improves with the depth until it overfits. However, it should also involve diversity. In the following video, you will learn how it is achieved in a random forest.

**VIDEO**

#### Random Forest

Qn: Is it possible to have duplicate rows in a bootstrapped sample?

Ans: *Yes, it is possible to have similar rows in the same bootstrapped sample. This is because bootstrap generated a new sample from the previous sample by replacement of values.*

As mentioned in the video, random selection of data points (bootstrap sample) and a random sample of features while splitting at every node make the base models diverse. This results in both diverse and acceptable models for ensemble learning; thus, random forest served as one of the best ML algorithms for a very long time. As mentioned by Pramod earlier, winners of many competitions used this technique, as it provided robust results.

#### Random Forest

Qn: You have learnt that Bagging suffers from a problem of feature dominance. How is the random forest algorithm able to overcome that?

Ans: *Random Forest models overcome the issue of feature dominance by generating many decision trees, and then aggregating the predictions of each individual tree to a single model prediction.*

Now, you are ready with a diverse and acceptable set of models. Next, you must aggregate the results of individual trees to obtain the final results. In the next video, you will learn how to do that for a classification problem.

**VIDEO**

In the random forest algorithm, you end up with different training subsets with overlapping data points. Hence, you have an overlapping testing set for each base model as well. Therefore, the algorithm aggregates the results of each model on every point. It takes a majority vote on the predictions made by models for every data point and provides the output as the class that has received the highest number of votes. In the case of regression, it will be the average of all the predicted values available for the particular data point.

Let's summarise the steps of the algorithm. Consider a random forest of 10 decision trees.

-   First, the algorithm will generate 10 bootstrapped samples from the sample data.
-   Next, each sample will be used to train a decision tree. Remember that the set of features used to split at each node of every tree is changing and is randomly selected. In this way, you create 10 decision trees.
-   Recall that in a decision tree, every data point passes from the root node to the bottom until it is classified in a leaf node. A similar process takes place in random forests as well while making predictions. Each data point passes through different trees in the ensemble that is built on different training and feature subsets.
-   The final outcomes of each tree are then combined either by taking the most frequent class prediction in case of a classification problem or by taking an average in case of a regression problem.

### Random Forest algorithm

![Random Forest Algorithm](https://i.ibb.co/QMsnpKQ/Random-Forest-Algorithm.jpg) 

It is worth reiterating that random forests have been much more successful than decision trees. In fact, as you learnt, ensembles are better than individual models (assuming diversity and acceptability). You can say that random forests are almost better than decision trees only if the trees are diverse and acceptable.

#### Building a Random Forest

Qn: What is the process of subsetting observations and features for each decision tree in a random forest? (Note: More than one option may be correct.)

- A random subset of observations is chosen, and every tree is built on that particular subset.

- A random subset of observations is chosen every time a new tree is built in a forest.

- A random subset of features is chosen every time a node is being split inside a tree.

- A random subset of features is chosen every time a new tree is built in a forest.

Ans: B & C. *A different random subset of observations is chosen, which is called the bootstrap sample, for each tree that is to be built in the forest. This is called bootstrapping. After the bootstrap sample is selected, tree building starts, and a random subset of features is selected at each node in order to split it. This is what makes random forests better than a simple bagging algorithm.*

#### Random Forest vs Bagging

Qn: How is random forest different from bagging? Select from the options given below.

- In a random forest, a random sample of features is chosen at each node split, which does not happen in bagging.

- In a random forest, a random sample of observations is chosen for each tree, which does not happen in bagging.

- In a random forest, a random sample of features is chosen for each tree, which does not happen in bagging.

- In a random forest, a random sample of observations is chosen at each split, which does not happen in bagging.

Ans: A. *Bagging includes the creation of different bootstrap samples for different models, and aggregating the results of the models. Random forests use this technique along with randomly selecting features at each node while splitting it.*

#### Aggregation in Random Forest

Qn: During bagging, or bootstrap aggregation, the test data point is passed through all the trees of the forest, and each tree makes its own prediction. How are these predictions aggregated in the case of a regression problem?

- Mode

- Mean

- Median

Ans: B. *The final prediction is the mean of all the predictions of the individual decision trees.*

#### Random Forests vs Decision Trees

Qn: In terms of accuracy, is a random forest always better than a decision tree?

- Yes

- No

Ans: B. *While it is well known that random forests are better than a single decision tree in terms of accuracy, it cannot be said that they are better than every possible decision tree. The only issue is that it is more difficult to build a decision tree that is better than a random forest. In fact, there may be several trees that provide better predictions on unseen data.*

In the next segment, you will learn how to measure the performance of the random forest model.