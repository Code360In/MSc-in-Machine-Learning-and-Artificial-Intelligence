# Introduction

In the previous session, you gain an understanding of the basic concepts of Apache Spark. You learnt about the different components in Spark and understood the differences between Spark and MapReduce. You were also introduced to the core abstraction of Spark: the RDDs.

## In this session

We will build on the programming aspect of Spark. You will learn how to load and process data in Spark RDDs using the PySpark API. The following topics will be covered in this session:

- Setting up Spark on an instance
- Creating RDDs
- Performing operations on RDDs

The main objective of this session is to provide you with hands-on experience of working on Spark's computation engine and data objects. While running this code, if you encounter errors, try to debug them on your own. This will give you valuable experience in handling Spark jobs.

The code provided in the videos in the upcoming segments must be typed and implemented simultaneously. Explanations are provided with the videos in the form of text along with some additional exercises. Ensure that you implement these in order to thoroughly understand the topics covered in the session.

## People you will hear from in this session

**Subject matter experts**

[Janakiram D](https://www.iitm.ac.in/info/fac/djram)  
Professor, IIT Madras

Dharanipragada Janakiram has completed his PhD from IIT Delhi and is currently a professor in the Department of Computer Science and Engineering, IIT Madras, where he heads and coordinates the research activities of the Distributed and Object Systems Lab. He was awarded the IBM Faculty Award in 2007 and the Yahoo Faculty Grant in 2009. He has guided 14 PhD students, over 36 MS (research) students and 70 MTech students. He has authored over 150 research papers and is the editor of six books. The Minimalistic Object Oriented Linux (MOOL) OS developed by him along with his students at the DOS lab is being distributed by CDAC as part of Bharat Operating System and Solutions (BOSS).
