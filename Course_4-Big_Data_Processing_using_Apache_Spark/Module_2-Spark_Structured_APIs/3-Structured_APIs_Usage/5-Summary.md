# Summary

Let’s quickly recall everything that you have learnt in this session:

1. The session began with the ability of spark to read from a lot of different data sources. Spark can read data from:
   
   - Files like parquet, CSV, JSON, etc 
   
   - Databases like HDFS and ODBC
     
     Spark can also write files in standard formats like Parquet, JSON etc. Making spark a very universal platform.  
      

2. Then, the SME explained which abstraction is used under what conditions. Choosing the appropriate abstraction depends on the situation and the features of the abstraction.

Let Sajan summarise your learnings from this module in the video below.

**VIDEO**

The module started with the limitations of RDDs and the structured APIs that were added to spark to overcome the limitations. You learnt about the structured APIs - dataframes datasets and sparkSQL. You learnt coding in these abstractions and solved some interesting problems in the spark environment. 

Next, you learnt about the major differentiating factor in RDDs and structured APIs - the catalyst optimiser. In this discussion, you explored its working using a small query. Then you used SparkUI to visualise the working of the catalyst optimiser.   

Then the next discussion was based around the details needed to actually use a spark:

1. Reading data from different data sources.

2. Choosing the appropriate abstraction.

Now that you have learnt about the usage of Spark, answer a few questions from the Graded Assessments segment at the end of this module.