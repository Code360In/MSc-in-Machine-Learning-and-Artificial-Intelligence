{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6339fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b303327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebef56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 17:58:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"GradedQuestions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba30009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"employees_1.json\", format = \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce33b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+\n",
      "|       DOB|   name|salary|\n",
      "+----------+-------+------+\n",
      "|1993-02-16|Michael|  3000|\n",
      "|1991-11-20|   Andy|  4500|\n",
      "|1991-11-28| Justin|  3500|\n",
      "|1993-05-21|  Borta|  4000|\n",
      "|1960-09-25|  David|  4800|\n",
      "|1966-09-19|  Sandy|  5100|\n",
      "|1996-05-31|  Peter|  1500|\n",
      "|1992-03-02|   John|  4700|\n",
      "+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d2018",
   "metadata": {},
   "source": [
    "The data type of the column salary and DOB is ___ and ____ respectively. Fill in the blanks with the correct options.\n",
    "\n",
    "- integer, string\n",
    "\n",
    "- string, integer\n",
    "\n",
    "- string, date\n",
    "\n",
    "- string, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548f5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOB: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784924c",
   "metadata": {},
   "source": [
    "Qn: In the previous question, you read in a data file with will all string columns. Now your task is to add new columns by converting the salary and date to the right data types. Form the following make the correct code segments to complete your task.\n",
    "\n",
    "More than one option can be correct.\n",
    "\n",
    "- `df = df.withColumn('salary_int',df[\"salary\"].cast(Integer()))`\n",
    "  \n",
    "- `df = df.withColumn('salary_int',df[\"salary\"].cast(IntegerType()))`\n",
    "  \n",
    "- `df = df.withColumn('DOB_date',df[\"DOB\"].cast(DateType()))`\n",
    "  \n",
    "- `df = df.withColumn('DOB_date',df[\"DOB\"].cast(Date()))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b8ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df = df.withColumn('salary_int',df[\"salary\"].cast(IntegerType()))\n",
    "df = df.withColumn('DOB_date',df[\"DOB\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df307a7",
   "metadata": {},
   "source": [
    "Qn: In the earlier question, you converted the salary and the date columns to the right data types. Now to save the dataframe with only the two corrected columns (salary_int, DOB_date) as a csv file on the EC2 instance, which of the following code will correct? More than one option can be correct. \n",
    "\n",
    "\n",
    "- df.write.csv('emplyee_corrected', Header = True)\n",
    "\n",
    "- df.select('salary_int','DOB_date').write.csv('emplyee_corrected.csv')\n",
    "\n",
    "- df.select('salary_int','DOB_date').write.option(\"header\", \"true\").csv('emplyee_corrected.csv')\n",
    "\n",
    "- df.write.option(\"header\", \"true\").csv('emplyee_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf43ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('salary_int','DOB_date').write.csv('emplyee_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58128bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('salary_int','DOB_date').write.option(\"header\", \"true\").csv('emplyee_corrected1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
