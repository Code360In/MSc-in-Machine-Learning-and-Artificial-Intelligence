# Introduction

The previous module was aimed at introducing Apache Spark and RDDs. You learnt the basic concepts of spark and its core abstraction - RDDs. You also saw how RDDs are different from the MapReduce framework. Apart from this, you also learnt how to set up Spark on an EC2 instance and explored different ways of loading and processing data.

In the next video, our SME, Sajan, will talk about the objective of this module.

**VIDEO**

## In this module

In this module, you will be introduced to structured APIs in Spark. We will see how these APIs are different from **R**esilient **D**istributed **D**atasets (RDDs). You will then dive into the jupyter notebook and perform EDA using the structured APIs. When running the code, you will notice how the code and its output both are more readable in the case of structured APIs. In the upcoming video, our SME will be taking you through the different topics that we will cover in this module.

**VIDEO**

An important differentiating feature of the structured APIs is the inbuilt optimiser. As the name suggests, it optimises the code for better performance. You will understand its working, both in theory and in practice in this module. Then you will explore **Spark UI**, a tool which can help visualise the working of the optimiser.

Then, you will be understanding the features of Spark that make it universal. You will learn how to read data from a lot of different data sources in Spark.

Finally, you will learn about choosing the right abstraction for your project.

## In this session

In this session, you will be introduced to structured APIs in Spark, i.e., dataframes. By the end of this session, you will have both theoretical knowledge and hands-on experience of dataframes and SparkSQL.

## People you will hear from in this session

**Subject matter expert**

[Sajan Kedia](https://in.linkedin.com/in/sajan-kedia-b06a6821)

Data Science Lead - Myntra

Sajan has completed his undergraduate and postgraduate in Computer Science Engineering from IIT, BHU. He heads the pricing team at Myntra, where he actively works on technologies like Data Science, Big Data, Spark and Machine learning. Presently, his work mainly involves the development of discounting strategies for all the products offered by Myntra.
