# Summary

You began this session by understanding the concepts of overfitting and underfitting using the below image.

![Underfitting Overfitting and Model Capacity Model Complexity](https://i.ibb.co/7b8RYdg/Underfitting-Overfitting-and-Model-Capacity-Model-Complexity.png)

In image A, you can see that the data was assumed to follow a linear trend and a linear model is fitted. However, on a closer look, it is clear that this model is poorly fitting the data. Such a model is said to be **underfitting**.

Now, if we increase the complexity of the model to such an extent that the model tries to fit all the points including the outliers in the data such as the case in image C. Such a model is said to be **overfitting**. 

After this, you learned that in the context of machine learning, these concepts of underfitting and overfitting can be generalized to any machine learning model by defining two parameters - Bias and Variance.

Bias essentially means ‘How far the model is from the optimal position’. Bias quantifies how accurate the model is likely to be on the future (test) data. A high bias suggests that the model has been generated by considering many assumptions and making the algorithm less complex. Hence, **high bias corresponds to underfitting**. An example of a High Bias algorithm is Linear Regression.

The ‘variance’ of a model is the variance (as learnt in inferential statistics) in the output on the same test data with respect to the changes in the training data. In other words, variance here refers to the degree of changes in the model itself with respect to changes in training data. **High variance corresponds to overfitting.** An example of a High Variance algorithm is Decision Trees.

For any model, their MSE (Mean Squared Error) can be decomposed into the following,

$$MSE=Bias^2+Variance+Irreducible\ error$$

As you can see from the above equation, for the same MSE, if the Bias decreases, Variance increases and vice versa. Hence, there is a trade-off between bias and variance in the ML modelling process as shown in the below image. 

![Bias Variance Tradeoff Graph2](https://i.ibb.co/NWDGNK7/Bias-Variance-Tradeoff-Graph2.png)

After this, you learned about regularization that is used to avoid overfitting your model on training data. Regularisation is a process used to create an optimally complex model, i.e., a model that is as simple as possible while performing well on the training data.

Regularization involves adding a new error part to your cost function thereby controlling the model complexity. You most common regularization techniques are as follows:

-   $L_2$ regularization / Ridge Regression / Tikhonov Regularization
    
-   $L_1$ regularization / LASSO
    
-   Elastic Net regularization = combination of $L_1$ and $L_2$
    

The key differences between Ridge and Lasso regression can be summarised in this table below.

![Ridge and Lasso Differences](https://i.ibb.co/x1nrgZL/Ridge-and-Lasso-Differences.png)

Elastic net regularization is a combination of ridge and lasso regression defined to provide a good balance between reducing overfitting without eliminating too many features from the model. The function is given by:

$$L_{elastic}=Loss+\lambda_2||w||_2+\lambda_1||w||_1$$

You also learned about K-fold Cross-Validation which can be used to detect overfitting. K-fold Cross Validation is a statistical technique that enables us to make extremely efficient use of available data. It divides the data into several pieces, or 'folds', and uses each piece as test data one at a time.

You also learned about GridSearchCV that can be used for finding the optimal hyperparameters for our model. 

Lastly, you looked at a Car Price prediction case study that used GridSearchCV for the find the optimal regularization parameters for ridge and lasso regression and find the optimal model that fits the data.

The lecture notes for this module are given below for your reference.

Download [Lecture Notes - Logistic Regression](../Logistic_Regression.pdf)

#### Takeaways

Qn: What are the major takeaways from this session?

Ans: *The validation set is used because hyperparameters need some unseen data to tune the model. Cross-Validation is a statistical technique which enables us to make extremely efficient use of available data.*
