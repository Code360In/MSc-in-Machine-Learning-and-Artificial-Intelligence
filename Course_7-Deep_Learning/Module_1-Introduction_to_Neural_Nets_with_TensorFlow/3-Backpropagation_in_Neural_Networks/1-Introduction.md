# Introduction

In the previous session, you learnt how information flows through a neural network in the forward direction. You learnt that this flow of information is known as feedforward and also learnt how to compute the feedforward output.

## In this session

You will learn about the process of training neural networks, called **backpropagation**. By the end of this session, you should be able to build your own neural network from scratch in NumPy. Let’s hear from Usha as she provides an overview of the topics that will be covered in this session:

**VIDEO**

In this session, the following topics will be covered:

1.  Loss function
2.  Gradient descent algorithm
3.  Backpropagation algorithm
4.  Updating weights and biases
5.  Gradient descent variants and batch in backpropagation
6.  Backpropagation Implementation on NumPy
7.  Regularization techniques

## Prerequisites

Go through session 'Differential Calculus' in the 'Math for Machine Learning' resource that has been shared with you previously in Machine Learning I. The knowledge of matrix operations and differential calculus will help you understand the session with much ease.

## People you will hear from in this session:

**Subject Matter Expert:**

[Usha Rengaraju](https://www.linkedin.com/in/usha-rengaraju-b570b7a2/)

Data Science Consultant

Usha currently works as a data science consultant at Infinite-Sum Modelling Inc. She has more than four years of experience in the field of deep learning and is the chapter lead for the TensorFlow User Group and Google Developers Group, Mysore. She is also the co-organiser of Women in Machine Learning & Data Science, Bengaluru Chapter.