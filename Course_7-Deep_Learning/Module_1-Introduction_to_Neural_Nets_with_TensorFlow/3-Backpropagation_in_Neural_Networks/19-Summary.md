# Summary

In this session, you learnt about the backpropagation algorithm. Let’s watch the next video for a summary of your learnings from this session.

**VIDEO**
  
The topics were covered in this session:

1.  You began this session by learning how to train a neural network.  
     
2.  You learnt how to compute the loss of a model and minimise this loss by adjusting the parameters (weights and biases) of the model.  
     
3.  Next, you learnt how to minimise the loss function using the gradient descent algorithm.  
     
4.  Further, you learnt about the backpropagation algorithm and how to update the parameters of a model using this algorithm by solving simple and complex problems.  
     
5.  You derived the equations for this algorithm and wrote the pseudocode using the same.  
     
6.  Next, you learnt about the different variants of gradient descent and understood the difference between them. The variants are as follows:
    1.  Batch gradient descent
    2.  Stochastic gradient descent
    3.  Mini-batch gradient descent  
         
7.  Further, you learnt how to backpropagate in batches. This can make your model efficient when working with huge data sets.  
     
8.  You implemented the backpropagation algorithm using the MNIST data set on NumPy.  
     
9.  After building and training your model, you learnt about the following regularization techniques:
    1.  L1 and L2 norms
    2.  Dropouts
    3.  Batch normalization

Download the MNIST implementation solution code file below: 

[MNIST_Solution](../MNIST_Solution.ipynb)