{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjn2AldPQjlO"
      },
      "source": [
        "In this assignment, you'll implement an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 103 µs (started: 2022-09-25 04:13:28 +05:30)\n"
          ]
        }
      ],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-q9-CshQjlR"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AlZQmaJkQjlR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 818 ms (started: 2022-09-25 04:13:29 +05:30)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmAxXn0QjlS"
      },
      "source": [
        "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xr2G_gaWQjlT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 296 µs (started: 2022-09-25 04:13:29 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def load_data():\n",
        "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
        "    f.seek(0)\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fvS3n2NQjlT"
      },
      "source": [
        "Let's see how the data looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Id3Wr75QjlT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 324 ms (started: 2022-09-25 04:13:29 +05:30)\n"
          ]
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pzg-93YQjlU",
        "outputId": "1dd68d81-b136-4be6-e4e8-7264d7eebbce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " array([5, 0, 4, ..., 8, 4, 8]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.79 ms (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfee2Z-TQjlU",
        "outputId": "700257ae-cdd3-4588-8a3e-8426a09a8045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n",
            "time: 320 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# shape of data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnOb0725QjlU",
        "outputId": "7c03f416-8ab5-4546-ae99-143c407d7881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "The target dataset is:[5 0 4 ... 8 4 8]\n",
            "The number of examples in the training dataset is:50000\n",
            "The number of points in a single input is:784\n",
            "time: 587 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "print(\"The feature dataset is:\" + str(training_data[0]))\n",
        "print(\"The target dataset is:\" + str(training_data[1]))\n",
        "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
        "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ_rsrfbQjlV"
      },
      "source": [
        "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zTbvYElYQjlV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 324 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def one_hot(j):\n",
        "    # input is the target dataset of shape (m,) where m is the number of data points\n",
        "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
        "    # Look at the next block of code for a better understanding of one hot encoding\n",
        "    n = j.shape[0]\n",
        "    new_array = np.zeros((10, n))\n",
        "    index = 0\n",
        "    for res in j:\n",
        "        new_array[res][index] = 1.0\n",
        "        index = index + 1\n",
        "    return new_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNGGrxyAQjlV",
        "outputId": "a1d77787-128e-4863-a867-0faf399e3a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.5 ms (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "print(data.shape)\n",
        "one_hot(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJUWtR8XQjlW"
      },
      "source": [
        "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dAZPIfj1QjlW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 425 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    \n",
        "    training_inputs = np.array(tr_d[0][:]).T\n",
        "    training_results = np.array(tr_d[1][:])\n",
        "    train_set_y = one_hot(training_results)\n",
        "    \n",
        "    validation_inputs = np.array(va_d[0][:]).T\n",
        "    validation_results = np.array(va_d[1][:])\n",
        "    validation_set_y = one_hot(validation_results)\n",
        "    \n",
        "    test_inputs = np.array(te_d[0][:]).T\n",
        "    test_results = np.array(te_d[1][:])\n",
        "    test_set_y = one_hot(test_results)\n",
        "    \n",
        "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s1pWI6ygQjlW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 311 ms (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z02PDycjQjlW",
        "outputId": "c5dba9a8-af58-4f75-e972-c7677fd20af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_set_x shape: (784, 50000)\n",
            "train_set_y shape: (10, 50000)\n",
            "test_set_x shape: (784, 10000)\n",
            "test_set_y shape: (10, 10000)\n",
            "time: 309 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRNiiUYeQjlX"
      },
      "source": [
        "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qZ6PXPZHQjlX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 941 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "y = pd.DataFrame(train_set_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "_322KgzgQjlX",
        "outputId": "db2546fb-f4df-4afc-ea46-472548c56ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The target dataset is:[5 0 4 ... 8 4 8]\n",
            "The one hot encoding dataset is:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>49990</th>\n",
              "      <th>49991</th>\n",
              "      <th>49992</th>\n",
              "      <th>49993</th>\n",
              "      <th>49994</th>\n",
              "      <th>49995</th>\n",
              "      <th>49996</th>\n",
              "      <th>49997</th>\n",
              "      <th>49998</th>\n",
              "      <th>49999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 50000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
              "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
              "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0  ...   \n",
              "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  ...   \n",
              "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
              "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  ...   \n",
              "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
              "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
              "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
              "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
              "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
              "\n",
              "   49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
              "0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
              "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
              "2    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
              "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
              "4    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
              "5    0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
              "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
              "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
              "8    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
              "9    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
              "\n",
              "[10 rows x 50000 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 14.1 ms (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "print(\"The target dataset is:\" + str(training_data[1]))\n",
        "print(\"The one hot encoding dataset is:\")\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ibnFa1QjlX"
      },
      "source": [
        "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "DEf0lMRGQjlX",
        "outputId": "eb572882-62d8-47ca-d042-ffc8b22118b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x17756a830>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 77.3 ms (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "index  = 1000\n",
        "k = train_set_x[:,index]\n",
        "k = k.reshape((28, 28))\n",
        "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
        "plt.imshow(k, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSrgnydBQjlY"
      },
      "source": [
        "# Feedforward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-GZbT4pQjlY"
      },
      "source": [
        "### sigmoid\n",
        "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the *sigmoid* function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kRVstNmOQjlY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 283 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(Z):\n",
        "    \n",
        "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
        "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
        "    \n",
        "    H = 1/(1+np.exp(-Z))\n",
        "    sigmoid_memory = Z\n",
        "    \n",
        "    return H, sigmoid_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1IsR5RPQjlZ",
        "outputId": "dd64d34b-84ba-490a-e43e-2c39017f9369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sigmoid(Z) = (array([[0.5       , 0.73105858],\n",
            "       [0.88079708, 0.95257413],\n",
            "       [0.98201379, 0.99330715],\n",
            "       [0.99752738, 0.99908895]]), array([[0, 1],\n",
            "       [2, 3],\n",
            "       [4, 5],\n",
            "       [6, 7]]))\n",
            "time: 403 µs (started: 2022-09-25 04:13:30 +05:30)\n"
          ]
        }
      ],
      "source": [
        "Z = np.arange(8).reshape(4,2)\n",
        "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YTdSuyQjlZ"
      },
      "source": [
        "### relu\n",
        "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **relu** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D6yupxhcQjlZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 262 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def relu(Z):\n",
        "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
        "    # relu_memory is stored as it is used later on in backpropagation\n",
        "    \n",
        "    H = np.maximum(0,Z)\n",
        "    \n",
        "    assert(H.shape == Z.shape)\n",
        "    \n",
        "    relu_memory = Z \n",
        "    return H, relu_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtQ3biLlQjlZ",
        "outputId": "edae9906-524c-4bd5-8712-ff6781f0fd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "relu(Z) = (array([[ 1,  3],\n",
            "       [ 0,  0],\n",
            "       [ 0,  7],\n",
            "       [ 9, 18]]), array([[ 1,  3],\n",
            "       [-1, -4],\n",
            "       [-5,  7],\n",
            "       [ 9, 18]]))\n",
            "time: 362 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
        "print (\"relu(Z) = \" + str(relu(Z)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUktPLfCQjla"
      },
      "source": [
        "### softmax\n",
        "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **softmax** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LZCZ13A2Qjla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 285 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def softmax(Z):\n",
        "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
        "    # softmax_memory is stored as it is used later on in backpropagation\n",
        "   \n",
        "    Z_exp = np.exp(Z)\n",
        "\n",
        "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
        "    \n",
        "    H = Z_exp/Z_sum  #normalising step\n",
        "    softmax_memory = Z\n",
        "    \n",
        "    return H, softmax_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZINXTQIcQjla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 194 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "Z = np.array([[11,19,10], [12, 21, 23]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavBomDOQjla",
        "outputId": "ad4ceff0-2e80-4d30-81fd-6032c09862ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n",
            " [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n",
            "[[11 19 10]\n",
            " [12 21 23]]\n",
            "time: 434 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "#Z = np.array(np.arange(30)).reshape(10,3)\n",
        "H, softmax_memory = softmax(Z)\n",
        "print(H)\n",
        "print(softmax_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiozvNUQQjlb"
      },
      "source": [
        "### initialize_parameters\n",
        "Let's now create a function **initialize_parameters** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **initialize_parameters** function initializes the parameters for all the layers in one `for` loop. \n",
        "\n",
        "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
        "\n",
        "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
        "\n",
        "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1b3n2LGfQjlb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 426 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def initialize_parameters(dimensions):\n",
        "\n",
        "    # dimensions is a list containing the number of neuron in each layer in the network\n",
        "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "    np.random.seed(2)\n",
        "    parameters = {}\n",
        "    L = len(dimensions)            # number of layers in the network + 1\n",
        "\n",
        "    for l in range(1, L): \n",
        "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
        "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tah2qf2_Qjlb",
        "outputId": "3463263d-6dcd-4468-e758-9112f1e00a16",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
            "  -0.09464469]\n",
            " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
            "   0.01179136]\n",
            " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
            "   0.00545832]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
            " [-0.28074571 -0.13967752  0.02641189]\n",
            " [ 0.10925169  0.06646016  0.08565535]\n",
            " [-0.11058228  0.03715795  0.13440124]\n",
            " [-0.16421272 -0.1153127   0.02013163]\n",
            " [ 0.13985659  0.07228733 -0.10717236]\n",
            " [-0.05673344 -0.03663499 -0.15460347]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "time: 1.87 ms (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "dimensions  = [784, 3,7,10]\n",
        "parameters = initialize_parameters(dimensions)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
        "# print(\"b3 = \" + str(parameters[\"b3\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVSW7F4yQjld"
      },
      "source": [
        "### layer_forward\n",
        "\n",
        "The function **layer_forward** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
        "\n",
        "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **sigmoid**, **relu** and **softmax** on **Z**.\n",
        "\n",
        "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DPk1SBO5Qjle"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 525 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
        "\n",
        "    # H_prev is of shape (size of previous layer, number of examples)\n",
        "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
        "    # b is bias vector of shape (size of the current layer, 1)\n",
        "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
        "\n",
        "    # H is the output of the activation function \n",
        "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z = np.dot(W, H_prev) + b \n",
        "        linear_memory = (H_prev, W, b)\n",
        "        H, activation_memory = sigmoid(Z)\n",
        " \n",
        "    elif activation == \"softmax\":\n",
        "        Z = np.dot(W, H_prev) + b \n",
        "        linear_memory = (H_prev, W, b)\n",
        "        H, activation_memory = softmax(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z = np.dot(W, H_prev) + b\n",
        "        linear_memory = (H_prev, W, b)\n",
        "        H, activation_memory = relu(Z)\n",
        "        \n",
        "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
        "    memory = (linear_memory, activation_memory)\n",
        "\n",
        "    return H, memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtXdZq_hQjle",
        "outputId": "502807b4-339a-4100-ce5f-dbeae92c25ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
              "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
              "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.56 ms (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "# l-1 has two neurons, l has three, m = 5\n",
        "# H_prev is (l-1, m)\n",
        "# W is (l, l-1)\n",
        "# b is (l, 1)\n",
        "# H should be (l, m)\n",
        "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
        "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
        "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
        "\n",
        "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
        "H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGPTN6yiQjlf"
      },
      "source": [
        "### L_layer_forward\n",
        "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **relu**. The last layer having a different activation **softmax** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DMGW8auRQjlf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 573 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def L_layer_forward(X, parameters):\n",
        "\n",
        "    # X is input data of shape (input size, number of examples)\n",
        "    # parameters is output of initialize_parameters()\n",
        "    \n",
        "    # HL is the last layer's post-activation value\n",
        "    # memories is the list of memory containing (for a relu activation, for example):\n",
        "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
        "    # - the memory of softmax forward (there is one, indexed L) \n",
        "\n",
        "    memories = []\n",
        "    H = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
        "    for l in range(1, L):\n",
        "        H_prev = H \n",
        "        \n",
        "        H, memory = layer_forward(H_prev, \n",
        "                                 parameters[\"W\" + str(l)], \n",
        "                                 parameters[\"b\" + str(l)], \n",
        "                                 activation='relu')\n",
        "        memories.append(memory)\n",
        "    \n",
        "    # Implement the final softmax layer\n",
        "    # HL here is the final prediction P as specified in the lectures\n",
        "    HL, memory = layer_forward(H,\n",
        "                              parameters[\"W\" + str(L)], \n",
        "                              parameters[\"b\" + str(L)], \n",
        "                              activation='softmax')\n",
        "    memories.append(memory)\n",
        "\n",
        "    assert(HL.shape == (10, X.shape[1]))\n",
        "            \n",
        "    return HL, memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSp12hEhQjlf",
        "outputId": "6840c1da-1ba7-407b-b9c3-1da0e08223a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 10)\n",
            "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n",
            " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n",
            " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n",
            " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n",
            " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n",
            " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n",
            " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n",
            " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n",
            " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n",
            " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n",
            "time: 589 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "# X is (784, 10)\n",
        "# parameters is a dict\n",
        "# HL should be (10, 10)\n",
        "x_sample = train_set_x[:, 10:20]\n",
        "print(x_sample.shape)\n",
        "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
        "print(HL[:, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOk3tIY7Qjlf"
      },
      "source": [
        "# Loss\n",
        "\n",
        "### compute_loss\n",
        "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **compute_loss** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qoGfkteVQjlf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 321 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def compute_loss(HL, Y):\n",
        "\n",
        "\n",
        "    # HL is probability matrix of shape (10, number of examples)\n",
        "    # Y is true \"label\" vector shape (10, number of examples)\n",
        "\n",
        "    # loss is the cross-entropy loss\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    loss = (-1./ m) * np.sum(np.multiply(Y, np.log(HL)))\n",
        "    \n",
        "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(loss.shape == ())\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrvcVJ1uQjlg",
        "outputId": "3080849d-77ba-4d37-eacf-51c8a92faeee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
            " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
            " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
            " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
            " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
            " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
            " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
            " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
            " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
            " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "0.8964600261334037\n",
            "time: 1.18 ms (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# sample\n",
        "# HL is (10, 5), Y is (10, 5)\n",
        "np.random.seed(2)\n",
        "HL_sample = np.random.rand(10,5)\n",
        "Y_sample = train_set_y[:, 10:15]\n",
        "print(HL_sample)\n",
        "print(Y_sample)\n",
        "\n",
        "print(compute_loss(HL_sample, Y_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCPmjHRyQjlg"
      },
      "source": [
        "# Backpropagation\n",
        "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
        "\n",
        "### sigmoid-backward\n",
        "You might remember that we had created **sigmoid** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wyeurCZvQjlg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 286 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def sigmoid_backward(dH, sigmoid_memory):\n",
        "    \n",
        "    # Implement the backpropagation of a sigmoid function\n",
        "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
        "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
        "    \n",
        "    Z = sigmoid_memory\n",
        "    \n",
        "    H = 1/(1+np.exp(-Z))\n",
        "    dZ = dH * H * (1-H)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbTDRuCUQjlg"
      },
      "source": [
        "### relu-backward\n",
        "You might remember that we had created **relu** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lT6rXQ8tQjlg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 302 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def relu_backward(dH, relu_memory):\n",
        "    \n",
        "    # Implement the backpropagation of a relu function\n",
        "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
        "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
        "    \n",
        "    Z = relu_memory\n",
        "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
        "    \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8JanEFSQjlh"
      },
      "source": [
        "### layer_backward\n",
        "\n",
        "**layer_backward** is a complimentary function of **layer_forward**. Like **layer_forward** calculates **H** using **W**, **H_prev** and **b**, **layer_backward** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **sigmoid_backward** and **relu_backward** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Wwyzln7sQjlh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 515 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def layer_backward(dH, memory, activation = 'relu'):\n",
        "    \n",
        "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
        "    # performs the backprop depending upon the activation function\n",
        "    \n",
        "\n",
        "    linear_memory, activation_memory = memory\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dH, activation_memory)\n",
        "        H_prev, W, b = linear_memory\n",
        "        m = H_prev.shape[1]\n",
        "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
        "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dH, activation_memory)\n",
        "        H_prev, W, b = linear_memory\n",
        "        m = H_prev.shape[1]\n",
        "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
        "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
        "    \n",
        "    return dH_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QBFxvHLQjlh",
        "outputId": "a64cf79a-8f29-4f81-d645-7793c7c356da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dH_prev is \n",
            " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
            " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
            "dW is \n",
            " [[1.67565336 1.56891359]\n",
            " [1.39137819 1.4143854 ]\n",
            " [1.3597389  1.43013369]]\n",
            "db is \n",
            " [[0.37345476]\n",
            " [0.34414727]\n",
            " [0.29074635]]\n",
            "time: 1.24 ms (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "# l-1 has two neurons, l has three, m = 5\n",
        "# H_prev is (l-1, m)\n",
        "# W is (l, l-1)\n",
        "# b is (l, 1)\n",
        "# H should be (l, m)\n",
        "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
        "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
        "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
        "\n",
        "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
        "np.random.seed(2)\n",
        "dH = np.random.rand(3,5)\n",
        "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
        "print('dH_prev is \\n' , dH_prev)\n",
        "print('dW is \\n' ,dW)\n",
        "print('db is \\n', db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9qbUAkQjlh"
      },
      "source": [
        "### L_layer_backward\n",
        "\n",
        "**L_layer_backward** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **layer_backward** here in the loop with the activation function as **relu**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "P-Iri1pFQjlh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 679 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def L_layer_backward(HL, Y, memories):\n",
        "    \n",
        "    # Takes the predicted value HL and the true target value Y and the \n",
        "    # memories calculated by L_layer_forward as input\n",
        "    \n",
        "    # returns the gradients calulated for all the layers as a dict\n",
        "\n",
        "    gradients = {}\n",
        "    L = len(memories) # the number of layers\n",
        "    m = HL.shape[1]\n",
        "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Perform the backprop for the last layer that is the softmax layer\n",
        "    current_memory = memories[-1]\n",
        "    linear_memory, activation_memory = current_memory\n",
        "    dZ = HL - Y\n",
        "    H_prev, W, b = linear_memory\n",
        "    gradients[\"dH\" + str(L-1)] = np.dot(linear_memory[1].T, dZ)\n",
        "    gradients[\"dW\" + str(L)] = (1. / m) * np.dot(dZ, H_prev.T) \n",
        "    gradients[\"db\" + str(L)] = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    \n",
        "    # Perform the backpropagation l-1 times\n",
        "    for l in reversed(range(L-1)):\n",
        "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
        "        current_memory = memories[l]\n",
        "        \n",
        "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)], current_memory, activation=\"relu\")\n",
        "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
        "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
        "        gradients[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMd736rqQjli",
        "outputId": "50510d43-eb69-42c1-d468-4decea64d23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dW3 is \n",
            " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
            "   0.        ]\n",
            " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
            "   0.        ]\n",
            " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
            "   0.        ]\n",
            " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
            "   0.        ]\n",
            " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
            "   0.        ]\n",
            " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
            "   0.        ]\n",
            " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
            "   0.        ]\n",
            " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
            "   0.        ]\n",
            " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
            "   0.        ]\n",
            " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
            "   0.        ]]\n",
            "db3 is \n",
            " [[ 0.10031756]\n",
            " [ 0.00460183]\n",
            " [-0.00142942]\n",
            " [-0.0997827 ]\n",
            " [ 0.09872663]\n",
            " [ 0.00536378]\n",
            " [-0.10124784]\n",
            " [-0.00191121]\n",
            " [-0.00359044]\n",
            " [-0.00104818]]\n",
            "dW2 is \n",
            " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]\n",
            " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]\n",
            " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]\n",
            " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]\n",
            " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]\n",
            " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "db2 is \n",
            " [[ 0.013979  ]\n",
            " [-0.01329383]\n",
            " [ 0.01275707]\n",
            " [-0.01052957]\n",
            " [ 0.03179224]\n",
            " [-0.00039877]\n",
            " [ 0.        ]]\n",
            "time: 1.63 ms (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "# X is (784, 10)\n",
        "# parameters is a dict\n",
        "# HL should be (10, 10)\n",
        "x_sample = train_set_x[:, 10:20]\n",
        "y_sample = train_set_y[:, 10:20]\n",
        "\n",
        "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
        "gradients  = L_layer_backward(HL, y_sample, memories)\n",
        "print('dW3 is \\n', gradients['dW3'])\n",
        "print('db3 is \\n', gradients['db3'])\n",
        "print('dW2 is \\n', gradients['dW2'])\n",
        "print('db2 is \\n', gradients['db2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_E9LHzGQjli"
      },
      "source": [
        "# Parameter Updates\n",
        "\n",
        "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "is7-osxaQjli"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 588 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
        "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    # returns updated weights after applying the gradient descent update\n",
        "\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]\n",
        "\n",
        "        \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElSFSm8jQjli"
      },
      "source": [
        "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Khoj29P4Qjli"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 180 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "dimensions = [784, 45, 10] #  three-layer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCe5gR9kQjlj"
      },
      "source": [
        "# Model\n",
        "\n",
        "### L_layer_model\n",
        "\n",
        "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have use for feedforward, computing the loss, backpropagation and updating the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1ZPd6m2RQjlj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 561 µs (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "# GRADED FUNCTION: L_layer_model\n",
        "\n",
        "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
        "    \n",
        "    # X and Y are the input training datasets\n",
        "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
        "    # returns updated parameters\n",
        "\n",
        "    np.random.seed(2)\n",
        "    losses = []                         # keep track of loss\n",
        "    \n",
        "    # Parameters initialization\n",
        "    parameters = initialize_parameters(dimensions)\n",
        " \n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation\n",
        "        HL, memories = L_layer_forward(X, parameters)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = compute_loss(HL, Y)\n",
        "    \n",
        "        # Backward propagation\n",
        "        gradients = L_layer_backward(HL, Y, memories)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "                \n",
        "        # Printing the loss every 100 training example\n",
        "        if print_loss and i % 100 == 0:\n",
        "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
        "            losses.append(loss)\n",
        "            \n",
        "    # plotting the loss\n",
        "    plt.plot(np.squeeze(losses))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uon86AZVQjlj"
      },
      "source": [
        "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x5RxBR0Qjlj",
        "outputId": "153ff14a-fa71-4b0b-ea99-cef5692a9594"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(784, 5000)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.32 ms (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "train_set_x_new = train_set_x[:,0:5000]\n",
        "train_set_y_new = train_set_y[:,0:5000]\n",
        "train_set_x_new.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaalzCR6Qjlj"
      },
      "source": [
        "Now, let's call the function L_layer_model on the dataset we have created. This will take 10-20 mins to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnYgCMqeQjlj",
        "outputId": "ab420b44-1c1e-482d-abf1-acba4cbe5e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.422624\n",
            "Loss after iteration 100: 2.129232\n",
            "Loss after iteration 200: 1.876095\n",
            "Loss after iteration 300: 1.604213\n",
            "Loss after iteration 400: 1.350205\n",
            "Loss after iteration 500: 1.144823\n",
            "Loss after iteration 600: 0.990554\n",
            "Loss after iteration 700: 0.876603\n",
            "Loss after iteration 800: 0.791154\n",
            "Loss after iteration 900: 0.725441\n",
            "Loss after iteration 1000: 0.673485\n",
            "Loss after iteration 1100: 0.631386\n",
            "Loss after iteration 1200: 0.596598\n",
            "Loss after iteration 1300: 0.567342\n",
            "Loss after iteration 1400: 0.542346\n",
            "Loss after iteration 1500: 0.520746\n",
            "Loss after iteration 1600: 0.501865\n",
            "Loss after iteration 1700: 0.485205\n",
            "Loss after iteration 1800: 0.470368\n",
            "Loss after iteration 1900: 0.457054\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjT0lEQVR4nO3dd3hUVf4G8Hdmkkz6hPRqEnoECdUQilIiARUMKm3VAFKEDSoCq/Lbpbi6RhGVRRBsEFxWAaW5oggEElqoIfSWEEKANAKZSW9zfn8ERoYU0u9M5v08zzww955753vnGvNy7rnnyoQQAkREREQmRC51AURERETNjQGIiIiITA4DEBEREZkcBiAiIiIyOQxAREREZHIYgIiIiMjkMAARERGRyWEAIiIiIpPDAEREREQmhwGIiOrNz88PEyZMkLoMIqI6YwAiklhUVBRkMhmOHTsmdSkmpaCgAAsXLkRMTIzUpej57rvvEBAQAEtLS7Rr1w5ffPFFrbctLi7GO++8A09PT1hZWSEoKAg7d+6ssu3BgwfRr18/WFtbw93dHW+88Qby8vL02kyYMAEymaza140bN3RtBwwYUGWboUOH1u+LIGpiZlIXQETG6+LFi5DLjfPfUQUFBXjvvfcAVPzyNgRfffUVpk2bhhdeeAGzZs3Cvn378MYbb6CgoADvvPPOQ7efMGECfv75Z8ycORPt2rVDVFQUnn76aezZswf9+vXTtUtISMDgwYMREBCAzz77DNevX8fixYtx+fJl/P7777p2r732GkJCQvQ+QwiBadOmwc/PD15eXnrrvL29ERkZqbfM09OzPl8FUdMTRCSp1atXCwDi6NGjktZRWloqiouLJa2hIepaf1ZWlgAgFixY0HRF1UFBQYFwcnISzzzzjN7yl156SdjY2Ijbt2/XuP3hw4cFAPHJJ5/olhUWFoo2bdqI4OBgvbbDhg0THh4eQq1W65Z98803AoD4448/avycffv2CQDiX//6l97yJ598UnTq1KnGbYkMiXH+043IBN24cQOvvvoq3NzcoFQq0alTJ6xatUqvTUlJCebPn48ePXpApVLBxsYG/fv3x549e/TaXb16FTKZDIsXL8aSJUvQpk0bKJVKnDt3DgsXLoRMJkNiYiImTJgABwcHqFQqTJw4EQUFBXr7eXAM0L3LeQcOHMCsWbPg4uICGxsbjBw5EllZWXrbarVaLFy4EJ6enrC2tsbAgQNx7ty5Wo0rqqn+2nwHV69ehYuLCwDgvffe012uWbhwoa7NhQsX8OKLL8LR0RGWlpbo2bMnfvnll4edpnrbs2cPsrOz8de//lVveUREBPLz87Ft27Yat//555+hUCgwdepU3TJLS0tMmjQJcXFxSE1NBQBoNBrs3LkTL7/8Muzt7XVtw8PDYWtriw0bNtT4OT/88ANkMhn+8pe/VLm+rKys0qU0IkPES2BERiAjIwO9e/eGTCbDjBkz4OLigt9//x2TJk2CRqPBzJkzAVT8cvv2228xbtw4TJkyBbm5ufjuu+8QGhqKI0eOoGvXrnr7Xb16NYqKijB16lQolUo4Ojrq1o0ePRr+/v6IjIxEfHw8vv32W7i6uuLjjz9+aL2vv/46WrVqhQULFuDq1atYsmQJZsyYgfXr1+vazJ07F4sWLcLw4cMRGhqKkydPIjQ0FEVFRbX+XqqqvzbfgYuLC1asWIHp06dj5MiReP755wEAXbp0AQCcPXsWffv2hZeXF959913Y2Nhgw4YNCAsLw8aNGzFy5Mga67pz5w7Ky8sfWr+1tTWsra0BACdOnAAA9OzZU69Njx49IJfLceLECbz88svV7uvEiRNo3769XqgBgMcffxxAxWUvHx8fnD59GmVlZZU+x8LCAl27dtXVUZXS0lJs2LABffr0gZ+fX6X1ly5dgo2NDUpKSuDm5oYpU6Zg/vz5MDc3r/5LIJKK1F1QRKauNpfAJk2aJDw8PMStW7f0lo8dO1aoVCpRUFAghBCirKys0mWgO3fuCDc3N/Hqq6/qliUnJwsAwt7eXmRmZuq1X7BggQCg114IIUaOHCmcnJz0lvn6+orx48dXOpaQkBCh1Wp1y9966y2hUChETk6OEEKI9PR0YWZmJsLCwvT2t3DhQgFAb59Vqan+2n4HNV0CGzx4sHjsscdEUVGRbplWqxV9+vQR7dq1q7E2ISq+FwAPfd3/2REREUKhUFS5PxcXFzF27NgaP7NTp05i0KBBlZafPXtWABArV64UQgjx008/CQBi7969ldqOGjVKuLu7V/sZ//vf/wQA8eWXX1Za9+qrr4qFCxeKjRs3iu+//16MGDFCABCjR4+usW4iqbAHiMjACSGwceNGjB49GkII3Lp1S7cuNDQU69atQ3x8PPr27QuFQgGFQgGg4hJTTk4OtFotevbsifj4+Er7fuGFF3SXgh40bdo0vff9+/fH5s2bodFoKvUyPGjq1KmQyWR6237++edISUlBly5dEB0djbKyskqXe15//XW9y1APU1X9df0OHnT79m3s3r0b//znP5Gbm4vc3FzdutDQUCxYsAA3btyoNAD4fv/9739RWFj40M9q3bq17u+FhYWwsLCosp2lpeVD91dYWAilUlnltvfW3/9ndW1r+pwffvgB5ubmGD16dKV13333nd77V155BVOnTsU333yDt956C717966xfqLmxgBEZOCysrKQk5ODr7/+Gl9//XWVbTIzM3V/X7NmDT799FNcuHABpaWluuX+/v6Vtqtq2T2PPPKI3vtWrVoBqLi887AAVNO2AJCSkgIAaNu2rV47R0dHXdvaqK7+unwHD0pMTIQQAvPmzcO8efOqbJOZmVljAOrbt+9DP+dBVlZWKCkpqXJdUVERrKysHrp9cXFxldveW3//n9W1re5z8vLysHXrVoSGhsLJyanGWu6ZPXs2vvnmG+zatYsBiAwOAxCRgdNqtQCAl19+GePHj6+yzb2xK2vXrsWECRMQFhaGv/3tb3B1dYVCoUBkZCSSkpIqbVfTL9V7vSgPEkI8tOaGbFsXVdVf1+/gQfe+7zlz5iA0NLTKNg8GtwdlZWXVagyQra0tbG1tAQAeHh4oLy9HZmYmXF1ddW1KSkqQnZ390NvJPTw89ObluSctLQ3An7eje3h46C1/sG11n7NlyxYUFBTgpZdeeuhx3ePj4wOgoleNyNAwABEZOBcXF9jZ2aG8vLzSnCwP+vnnn9G6dWts2rRJ7xLUggULmrrMOvH19QVQ0dtyf69Mdna2rpeovmr7Hdy/7n73LkuZm5s/9PuuTq9evXS9XDVZsGCB7pLfvQHqx44dw9NPP61rc+zYMWi12koD2B/UtWtX7Nmzp9IlysOHD+vtv3PnzjAzM8OxY8f0LmWVlJQgISGhystbQMVlPVtbW4wYMeKhx3XPlStXAKDay6xEUuJt8EQGTqFQ4IUXXsDGjRtx5syZSuvvv738Xs/L/T0thw8fRlxcXNMXWgeDBw+GmZkZVqxYobd82bJlDd53bb+De3df5eTk6C13dXXFgAED8NVXX1XZS/Lg7fxV+e9//4udO3c+9BUeHq7bZtCgQXB0dKz0naxYsQLW1tZ45plndMtu3bqFCxcu6E1L8OKLL6K8vFzvMmlxcTFWr16NoKAgXW+MSqVCSEgI1q5dqze+6T//+Q/y8vIwatSoKo95165dGDlypO57u59Go6l0SU0IgQ8++AAAqu1JI5ISe4CIDMSqVauwffv2SsvffPNNfPTRR9izZw+CgoIwZcoUPProo7h9+zbi4+Oxa9cu3SWGZ599Fps2bcLIkSPxzDPPIDk5GStXrsSjjz5qUHOzuLm54c0338Snn36KESNGYOjQoTh58iR+//13ODs7V9s7Uxu1/Q6srKzw6KOPYv369Wjfvj0cHR3RuXNndO7cGcuXL0e/fv3w2GOPYcqUKWjdujUyMjIQFxeH69ev4+TJkzXWUN8xQO+//z4iIiIwatQohIaGYt++fVi7di3+9a9/6U1RsGzZMrz33nvYs2ePbhbroKAgjBo1CnPnzkVmZibatm2LNWvW4OrVq5UGKP/rX/9Cnz598OSTT2Lq1Km4fv06Pv30UwwZMqTKR1esX78eZWVl1V7+io+Px7hx4zBu3Di0bdsWhYWF2Lx5Mw4cOICpU6eie/fudf4+iJqcdDegEZEQf946Xt0rNTVVCCFERkaGiIiIED4+PsLc3Fy4u7uLwYMHi6+//lq3L61WKz788EPh6+srlEql6Natm/j111/F+PHjha+vr67dvdvI7581+J57t8FnZWVVWWdycrJuWXW3wT94S/+ePXsEALFnzx7dsrKyMjFv3jzh7u4urKysxKBBg8T58+eFk5OTmDZtWo3fWU311/Y7EEKIgwcPih49eggLC4tKt6UnJSWJ8PBw4e7uLszNzYWXl5d49tlnxc8//1xjbQ319ddfiw4dOggLCwvRpk0b8fnnn+tNKSDEn+fo/u9TiIqZn+fMmSPc3d2FUqkUvXr1Etu3b6/yc/bt2yf69OkjLC0thYuLi4iIiBAajabKtr179xaurq6irKysyvVXrlwRo0aNEn5+fsLS0lJYW1uLHj16iJUrV1aqnchQyIRo5FGJRET1lJOTg1atWuGDDz7A3//+d6nLIaIWjGOAiEgSVc03s2TJEgCG83BSImq5OAaIiCSxfv163dPKbW1tsX//fvz4448YMmRIvcbQEBHVBQMQEUmiS5cuMDMzw6JFi6DRaHQDo+/dOURE1JQ4BoiIiIhMDscAERERkclhACIiIiKTI+kYoMjISGzatAkXLlyAlZUV+vTpg48//hgdOnSodpuoqChMnDhRb5lSqdQ98A+omIF0wYIF+Oabb5CTk4O+fftixYoVaNeuXa3q0mq1uHnzJuzs7Bo0IRsRERE1HyEEcnNz4enpCbm85j4eSQNQbGwsIiIi0KtXL5SVleH//u//MGTIEJw7dw42NjbVbmdvb4+LFy/q3j8YUhYtWoSlS5dizZo18Pf3x7x58xAaGopz587B0tLyoXXdvHlTN208ERERGZfU1FR4e3vX2MagBkFnZWXB1dUVsbGxeOKJJ6psExUVhZkzZ1Z6fs89Qgh4enpi9uzZmDNnDgBArVbDzc0NUVFRGDt27EPrUKvVcHBwQGpqqt5DBYmIiMhwaTQa+Pj4ICcnByqVqsa2BnUbvFqtBgC9Z95UJS8vD76+vtBqtejevTs+/PBDdOrUCQCQnJyM9PR0vac4q1QqBAUFIS4urlYB6F6Pkr29PQMQERGRkanN8BWDGQSt1Woxc+ZM9O3bF507d662XYcOHbBq1Sps3boVa9euhVarRZ8+fXD9+nUAQHp6OoCKhy3ez83NTbfuQcXFxdBoNHovIiIiarkMpgcoIiICZ86cwf79+2tsFxwcjODgYN37Pn36ICAgAF999RXef//9en12ZGQk3nvvvXptS0RERMbHIHqAZsyYgV9//RV79ux56KClB5mbm6Nbt25ITEwEALi7uwMAMjIy9NplZGTo1j1o7ty5UKvVuldqamo9joKIiIiMhaQBSAiBGTNmYPPmzdi9ezf8/f3rvI/y8nKcPn0aHh4eAAB/f3+4u7sjOjpa10aj0eDw4cN6PUf3UyqVuvE+HPdDRETU8kl6CSwiIgI//PADtm7dCjs7O90YHZVKBSsrKwBAeHg4vLy8EBkZCQD45z//id69e6Nt27bIycnBJ598gpSUFEyePBlAxcCnmTNn4oMPPkC7du10t8F7enoiLCxMkuMkIiIiwyJpAFqxYgUAYMCAAXrLV69ejQkTJgAArl27pjeZ0Z07dzBlyhSkp6ejVatW6NGjBw4ePIhHH31U1+btt99Gfn4+pk6dipycHPTr1w/bt2+v1RxARERE1PIZ1DxAhkKj0UClUkGtVvNyGBERkZGoy+9vgxgETURERNScGICIiIjI5DAAERERkclhACIiIiKTwwBEREREJocBqJmdvq7GrbxiqcsgIiIyaQxAzehf285h+LL9+G5/stSlEBERmTQGoGbUy88RALA2LgWaolKJqyEiIjJdDEDNKCTADe1cbZFbXIb/xKVIXQ4REZHJYgBqRnK5DNMHtAEArD6QjKLScokrIiIiMk0MQM1seKAnvFtZ4VZeCTYcS5W6HCIiIpPEANTMzBVyvPZEawDAV7FXUFqulbgiIiIi08MAJIFRPX3gbGuBGzmF+CXhptTlEBERmRwGIAlYmivwaj9/AMCK2CRotULiioiIiEwLA5BEXu7tCzulGRIz87DzfIbU5RAREZkUBiCJ2Fua45VgXwDAlzFJEIK9QERERM2FAUhCr/bzh9JMjpOpOYhLypa6HCIiIpPBACQhZ1slxvbyAVDRC0RERETNgwFIYlOeaA0zuQz7E2/hZGqO1OUQERGZBAYgiXm3ssaIrp4AgC9jEiWuhoiIyDQwABmA6U9WPB7jj7MZSMzMlbgaIiKilo8ByAC0c7PDkEfdAAArYq5IXA0REVHLxwBkIP46sC0AYGvCDdzIKZS4GiIiopaNAchAdPVxQJ82TijTCnyzl71ARERETYkByID8dUBFL9C6o9eQnVcscTVEREQtFwOQAenb1gmB3ioUlWqx+sBVqcshIiJqsRiADIhMJsP0u71Aa+KuIreoVOKKiIiIWiYGIAMz5FE3tHW1RW5RGdYeuiZ1OURERC0SA5CBkctlmHZ3XqDv9iejqLRc4oqIiIhaHgYgA/RcV094OVjhVl4xfjp+XepyiIiIWhwGIANkrpBjSn9/AMDXe5NQVq6VuCIiIqKWhQHIQI3p9QicbCyQersQv55Kk7ocIiKiFkXSABQZGYlevXrBzs4Orq6uCAsLw8WLF2vc5ptvvkH//v3RqlUrtGrVCiEhIThy5IhemwkTJkAmk+m9hg4d2pSH0uisLBR4tV9FL9CKmCRotULiioiIiFoOSQNQbGwsIiIicOjQIezcuROlpaUYMmQI8vPzq90mJiYG48aNw549exAXFwcfHx8MGTIEN27c0Gs3dOhQpKWl6V4//vhjUx9Oo3u5ty9slWa4mJGL3RcypS6HiIioxZAJIQymayErKwuurq6IjY3FE088UattysvL0apVKyxbtgzh4eEAKnqAcnJysGXLlnrVodFooFKpoFarYW9vX699NJaPfr+AlbFJ6PaIAzZN7wOZTCZpPURERIaqLr+/DWoMkFqtBgA4OjrWepuCggKUlpZW2iYmJgaurq7o0KEDpk+fjuzs7Gr3UVxcDI1Go/cyFK/284OFmRwnruXg0JXbUpdDRETUIhhMANJqtZg5cyb69u2Lzp0713q7d955B56enggJCdEtGzp0KL7//ntER0fj448/RmxsLIYNG4by8qrn1ImMjIRKpdK9fHx8Gnw8jcXVzhKje3oDAL6MSZS4GiIiopbBYC6BTZ8+Hb///jv2798Pb2/vWm3z0UcfYdGiRYiJiUGXLl2qbXflyhW0adMGu3btwuDBgyutLy4uRnHxnw8f1Wg08PHxMYhLYACQersAAxbHoFwr8L8Z/fCYt0rqkoiIiAyO0V0CmzFjBn799Vfs2bOn1uFn8eLF+Oijj7Bjx44aww8AtG7dGs7OzkhMrLoHRalUwt7eXu9lSHwcrTG8iwcAYEUse4GIiIgaStIAJITAjBkzsHnzZuzevRv+/v612m7RokV4//33sX37dvTs2fOh7a9fv47s7Gx4eHg0tGTJ3HtI6u9n0pGUlSdxNURERMZN0gAUERGBtWvX4ocffoCdnR3S09ORnp6OwsJCXZvw8HDMnTtX9/7jjz/GvHnzsGrVKvj5+em2ycurCAV5eXn429/+hkOHDuHq1auIjo7Gc889h7Zt2yI0NLTZj7GxdHC3Q0iAG4QAvopNkrocIiIioyZpAFqxYgXUajUGDBgADw8P3Wv9+vW6NteuXUNaWpreNiUlJXjxxRf1tlm8eDEAQKFQ4NSpUxgxYgTat2+PSZMmoUePHti3bx+USmWzH2Nj+uvAioekboq/gZs5hQ9pTURERNUxmEHQhsSQ5gF60Niv43Doym1M7OuHBcM7SV0OERGRwTC6QdBUe3+9OxZo3ZFU3M4vkbgaIiIi48QAZGT6t3NGZy97FJaWI+pAstTlEBERGSUGICMjk8l0vUBRB68ir7hM4oqIiIiMDwOQEQrt5I7WzjbQFJXhh8MpUpdDRERkdBiAjJBCLsO0ARV3hH27LxlFpVU/4oOIiIiqxgBkpMK6esFDZYnM3GJsjL8udTlERERGhQHISFmYyTGlf2sAwFexV1BWrpW4IiIiIuPBAGTExj7ug1bW5rh2uwDbTqc9fAMiIiICwABk1KwtzDCxb8Xz01bEJIFzWhIREdUOA5CRGx/sBxsLBS6k52LPxUypyyEiIjIKDEBGTmVtjpd6+wIAvtzDh6QSERHVBgNQCzC5nz8sFHIcS7mDI8m3pS6HiIjI4DEAtQCu9pZ4sac3AODLmESJqyEiIjJ8DEAtxGtPtIZcBsRczMKZG2qpyyEiIjJoDEAthK+TDUYEegIAPt5+QeJqiIiIDBsDUAsye0gHmCtk2Hf5FvZeypK6HCIiIoPFANSC+DhaIzzYDwDw4W/nUa7lvEBERERVYQBqYWYMbAs7SzNcSM/F5hM3pC6HiIjIIDEAtTCtbCwwY2BbAMCnOy7ySfFERERVYABqgcb38YOXgxXS1EVYdSBZ6nKIiIgMDgNQC2RprsCc0PYAgBV7knA7v0TiioiIiAwLA1AL9VygFx71sEducRmWRl+WuhwiIiKDwgDUQsnlMvzf0wEAgLWHUnD1Vr7EFRERERkOBqAWrF87ZzzZ3gVlWoFPdlyUuhwiIiKDwQDUwr07rCNkMmDbqTScuHZH6nKIiIgMAgNQCxfgYY8Xu1c8KDXytwsQgpMjEhERMQCZgFlD2kNpJseRq7ex81yG1OUQERFJjgHIBHiorDCpnz8A4KPtF1BWrpW4IiIiImkxAJmIaQPawNHGAley8rH+WKrU5RAREUmKAchE2Fua441BFY/I+HznZeQVl0lcERERkXQYgEzIX4J84edkjVt5xfhm7xWpyyEiIpIMA5AJsTCT4+2hHQEA3+y7gkxNkcQVERERSUPSABQZGYlevXrBzs4Orq6uCAsLw8WLD5+w76effkLHjh1haWmJxx57DL/99pveeiEE5s+fDw8PD1hZWSEkJASXL/NxEAAwrLM7uj3igIKScny+i98JERGZJkkDUGxsLCIiInDo0CHs3LkTpaWlGDJkCPLzq39sw8GDBzFu3DhMmjQJJ06cQFhYGMLCwnDmzBldm0WLFmHp0qVYuXIlDh8+DBsbG4SGhqKoiD0eMtmfj8hYf/QaEjNzJa6IiIio+cmEAc2Ml5WVBVdXV8TGxuKJJ56oss2YMWOQn5+PX3/9Vbesd+/e6Nq1K1auXAkhBDw9PTF79mzMmTMHAKBWq+Hm5oaoqCiMHTv2oXVoNBqoVCqo1WrY29s3zsEZmKnfH8OOcxkICXDFt+N7SV0OERFRg9Xl97dBjQFSq9UAAEdHx2rbxMXFISQkRG9ZaGgo4uLiAADJyclIT0/Xa6NSqRAUFKRr86Di4mJoNBq9V0v3zrCOUMhl2HU+E4euZEtdDhERUbMymACk1Woxc+ZM9O3bF507d662XXp6Otzc3PSWubm5IT09Xbf+3rLq2jwoMjISKpVK9/Lx8WnIoRiFNi62GPd4xXFG/naej8ggIiKTYjABKCIiAmfOnMG6deua/bPnzp0LtVqte6WmmsZEgW8Obg8bCwVOXlfj11NpUpdDRETUbAwiAM2YMQO//vor9uzZA29v7xrburu7IyND/3lWGRkZcHd3162/t6y6Ng9SKpWwt7fXe5kCFzslXnuyDQBg0R8XUFxWLnFFREREzUPSACSEwIwZM7B582bs3r0b/v7+D90mODgY0dHRest27tyJ4OBgAIC/vz/c3d312mg0Ghw+fFjXhv40ub8/XOyUSL1diLWHrkldDhERUbOQNABFRERg7dq1+OGHH2BnZ4f09HSkp6ejsLBQ1yY8PBxz587VvX/zzTexfft2fPrpp7hw4QIWLlyIY8eOYcaMGQAqbvOeOXMmPvjgA/zyyy84ffo0wsPD4enpibCwsOY+RINnbWGGWU+1BwB8sfsy1IWlEldERETU9CQNQCtWrIBarcaAAQPg4eGhe61fv17X5tq1a0hL+3N8Sp8+ffDDDz/g66+/RmBgIH7++Wds2bJFb+D022+/jddffx1Tp05Fr169kJeXh+3bt8PS0rJZj89YjOrhjXautsgpKMWKmCSpyyEiImpyBjUPkKEwhXmAHhR9PgOT1hyDhZkce+YMgJeDldQlERER1YnRzgNE0hnU0RW9WzuipEyLT3c8/HEkRERExowBiABUjJ2aO6ziERmbT9zA2ZtqiSsiIiJqOgxApBPo44DhgZ4QAvjo9wtSl0NERNRkGIBIz9uhHWCukGHf5VvYeylL6nKIiIiaBAMQ6fFxtEZ4sB8A4MPfzqNcyzHyRETU8jAAUSWvD2oLe0szXEjPxeYTN6Quh4iIqNExAFElDtYWiBjYFgDw6Y6LKCrlIzKIiKhlYQCiKo3v4wcvByukqYuw6kCy1OUQERE1KgYgqpKluQJzQisekbFiTxJu55dIXBEREVHjYQCiaj0X6IVOnvbILS7D0ujLUpdDRETUaBiAqFpyuQz/93TF5IhrD6Xg6q18iSsiIiJqHAxAVKO+bZ3xZHsXlGkFPuEjMoiIqIVgAKKHendYR8hkwLZTaThx7Y7U5RARETUYAxA9VICHPV7s7g2gYnJEITg5IhERGTcGIKqVWUPaQ2kmx9GrdxDDR2QQEZGRYwCiWvFQWSE82BcA8PnOS+wFIiIio8YARLU27ck2sLZQ4NR1NXadz5S6HCIionpjAKJac7JVYnwfPwDAZzsvQcsHpRIRkZFiAKI6mdq/NWyVZjifpsH2s+lSl0NERFQvDEBUJ61sLPBqP38AFWOBytkLRERERogBiOpsUj9/2Fua4XJmHn49dVPqcoiIiOqMAYjqTGVljin9WwMA/r3rMsrKtRJXREREVDcMQFQvE/v5o5W1Oa7cysfWBPYCERGRcWEAonqxVZrhtSfbAAD+HX0ZpewFIiIiI8IARPUWHuwLZ1sLXLtdgI3Hr0tdDhERUa0xAFG9WVuYYdrdXqAvdieiuKxc4oqIiIhqhwGIGuTl3r5ws1fiRk4hNhxjLxARERkHBiBqEEtzBSIGtgUALN+diKJS9gIREZHhYwCiBhvTyweeKkuka4rww+FrUpdDRET0UAxA1GBKMwVmDGoHAPgyJgmFJewFIiIiw8YARI1iVE9v+Dha4VZeMf5z6KrU5RAREdWIAYgahblCjtfv9gKtjL2CvOIyiSsiIiKqnqQBaO/evRg+fDg8PT0hk8mwZcuWGttPmDABMpms0qtTp066NgsXLqy0vmPHjk18JAQAz3fzgr+zDW7nl2DNwatSl0NERFQtSQNQfn4+AgMDsXz58lq1//e//420tDTdKzU1FY6Ojhg1apReu06dOum1279/f1OUTw8wU8jx5uCKXqCv916BpqhU4oqIiIiqZiblhw8bNgzDhg2rdXuVSgWVSqV7v2XLFty5cwcTJ07Ua2dmZgZ3d/dGq5Nqb3igJ5btSURiZh5W7U/GzJD2UpdERERUiVGPAfruu+8QEhICX19fveWXL1+Gp6cnWrdujZdeegnXrtV8a3ZxcTE0Go3ei+pHIZdhZkhFL9B3+5KRU1AicUVERESVGW0AunnzJn7//XdMnjxZb3lQUBCioqKwfft2rFixAsnJyejfvz9yc3Or3VdkZKSud0mlUsHHx6epy2/Rnu7sgY7udsgtLsM3+65IXQ4REVElRhuA1qxZAwcHB4SFhektHzZsGEaNGoUuXbogNDQUv/32G3JycrBhw4Zq9zV37lyo1WrdKzU1tYmrb9nkchneeqri0tfqA1dxO5+9QEREZFiMMgAJIbBq1Sq88sorsLCwqLGtg4MD2rdvj8TExGrbKJVK2Nvb672oYYY86obOXvYoKCnHV7FJUpdDRESkxygDUGxsLBITEzFp0qSHts3Ly0NSUhI8PDyaoTK6RyaTYdbdXqA1cVeRmVskcUVERER/kjQA5eXlISEhAQkJCQCA5ORkJCQk6AYtz507F+Hh4ZW2++677xAUFITOnTtXWjdnzhzExsbi6tWrOHjwIEaOHAmFQoFx48Y16bFQZQM7uKKrjwOKSrVYEcNeICIiMhySBqBjx46hW7du6NatGwBg1qxZ6NatG+bPnw8ASEtLq3QHl1qtxsaNG6vt/bl+/TrGjRuHDh06YPTo0XBycsKhQ4fg4uLStAdDlchkMsweUtEL9N/D15CuZi8QEREZBpkQQkhdhKHRaDRQqVRQq9UcD9RAQgiM/ioOR6/ewSu9ffF+WOVeOyIiosZQl9/fRjkGiIxHxVigDgCAdUev4fqdAokrIiIiYgCiZhDcxgl92jihtFxg2e7q78YjIiJqLgxA1Czu3RH20/HrSMnOl7gaIiIydQxA1Cx6+jniifYuKNcKLI1mLxAREUmLAYiazb1eoM0nriMpK0/iaoiIyJQxAFGz6erjgJAAV2gF8O9dl6Uuh4iITBgDEDWrmSEVvUD/O3UTlzKqf0AtERFRU2IAombV2UuFoZ3cIQSwZNclqcshIiITxQBEze6tp9pDJgN+O52OszfVUpdDREQmiAGIml0Hdzs828UTALCEY4GIiEgCDEAkiTcHt4NcBuw8l4FT13OkLoeIiEwMAxBJoq2rLcK6egEAPtvJsUBERNS8GIBIMm8MbgeFXIaYi1k4nnJH6nKIiMiEMACRZPycbfBid28AwOfsBSIiombEAESSmjGoLcwVMuxPvIXDV7KlLoeIiEwEAxBJysfRGqN7+gAAPt15CUIIiSsiIiJTwABEkpsxqC0sFHIcSb6NA4nsBSIioqbHAESS81BZ4S9BjwAAFu+4yF4gIiJqcgxAZBD+OrANrMwVSEjNwfYz6VKXQ0RELRwDEBkEVztLTHmiNQDg4+0XUFqulbgiIiJqyRiAyGBMfaI1nG0tcDW7AD8euSZ1OURE1IIxAJHBsFWa4c2Q9gCAf++6jNyiUokrIiKilooBiAzK2F4+aO1sg+z8Eny994rU5RARUQvFAEQGxVwhx9tDOwIAvtl3BRmaIokrIiKilogBiAxOaCc39PBthaJSLR+RQURETYIBiAyOTCbD/z1d0Qu04VgqLmXkSlwRERG1NAxAZJB6+DoitJMbtAL4+PcLUpdDREQtDAMQGay3h3aEQi5D9IVMHOKDUomIqBExAJHBauNii3GPVzwoNfK383xEBhERNRoGIDJobw5uD2sLBU5eV2Pb6TSpyyEiohaCAYgMmoudEq890QYAsGj7RZSU8REZRETUcJIGoL1792L48OHw9PSETCbDli1bamwfExMDmUxW6ZWerv/wzOXLl8PPzw+WlpYICgrCkSNHmvAoqKlN7u8PFzslrt0uwH8Pp0hdDhERtQCSBqD8/HwEBgZi+fLlddru4sWLSEtL071cXV1169avX49Zs2ZhwYIFiI+PR2BgIEJDQ5GZmdnY5VMzsVGa4a27j8hYGn0ZGj4ig4iIGkjSADRs2DB88MEHGDlyZJ22c3V1hbu7u+4ll/95GJ999hmmTJmCiRMn4tFHH8XKlSthbW2NVatWNXb51IxG9/RGGxcb3CkoxcqYJKnLISIiI2eUY4C6du0KDw8PPPXUUzhw4IBueUlJCY4fP46QkBDdMrlcjpCQEMTFxUlRKjUSM4Uc7w4LAAB8tz8ZaepCiSsiIiJjZlQByMPDAytXrsTGjRuxceNG+Pj4YMCAAYiPjwcA3Lp1C+Xl5XBzc9Pbzs3NrdI4ofsVFxdDo9HovcjwhAS44nE/RxSXafHZDj4ig4iI6q9eAWjNmjXYtm2b7v3bb78NBwcH9OnTBykpTTdItUOHDnjttdfQo0cP9OnTB6tWrUKfPn3w+eefN2i/kZGRUKlUupePj08jVUyNSSaT4d27j8j4Of46LqQzqBIRUf3UKwB9+OGHsLKyAgDExcVh+fLlWLRoEZydnfHWW281aoEP8/jjjyMxMREA4OzsDIVCgYyMDL02GRkZcHd3r3Yfc+fOhVqt1r1SU1ObtGaqv+6PtMLTj7lD8BEZRETUAPUKQKmpqWjbti0AYMuWLXjhhRcwdepUREZGYt++fY1a4MMkJCTAw8MDAGBhYYEePXogOjpat16r1SI6OhrBwcHV7kOpVMLe3l7vRYbrb6EdYSaXYc/FLBxMvCV1OUREZITqFYBsbW2RnV3xbKYdO3bgqaeeAgBYWlqisLD2g1Pz8vKQkJCAhIQEAEBycjISEhJw7do1ABU9M+Hh4br2S5YswdatW5GYmIgzZ85g5syZ2L17NyIiInRtZs2ahW+++QZr1qzB+fPnMX36dOTn52PixIn1OVQyQP7ONngp6BEAQOTvF6DV8hEZRERUN2b12eipp57C5MmT0a1bN1y6dAlPP/00AODs2bPw8/Or9X6OHTuGgQMH6t7PmjULADB+/HhERUUhLS1NF4aAiru8Zs+ejRs3bsDa2hpdunTBrl279PYxZswYZGVlYf78+UhPT0fXrl2xffv2SgOjybi9PrgdNsbfwOkbavzv1E0819VL6pKIiMiIyEQ9njCZk5ODf/zjH0hNTcX06dMxdOhQAMCCBQtgYWGBv//9741eaHPSaDRQqVRQq9W8HGbAlu2+jMU7LsG7lRWiZz8JpZlC6pKIiEhCdfn9Xa8A1NIxABmHwpJyDFi8BxmaYvzjmQBM7t9a6pKIiEhCdfn9Xa8xQNu3b8f+/ft175cvX46uXbviL3/5C+7cuVOfXRLVmZWFArOeqnhExhe7E6Eu4CMyiIioduoVgP72t7/pJgs8ffo0Zs+ejaeffhrJycm6cTxEzeGF7t5o72YLdWEpvoxNlLocIiIyEvUKQMnJyXj00UcBABs3bsSzzz6LDz/8EMuXL8fvv//eqAUS1aTiERkVkyOuPnAVN3L4iAwiInq4egUgCwsLFBQUAAB27dqFIUOGAAAcHR35GAlqdgM7uCLI3xElZVp8uuOi1OUQEZERqFcA6tevH2bNmoX3338fR44cwTPPPAMAuHTpEry9vRu1QKKHkclkmPt0xYNSN5+4gXM3GcKJiKhm9QpAy5Ytg5mZGX7++WesWLECXl4Vc7D8/vvvulviiZpTVx8HPNvFA0IAH23nIzKIiKhmvA2+CrwN3jilZOcj5LNYlJYL/GfS4+jfzkXqkoiIqBnV5fd3vWaCBoDy8nJs2bIF58+fBwB06tQJI0aMgELByehIGr5ONni5ty9WH7iKyN8uoO/rzpDLZVKXRUREBqhel8ASExMREBCA8PBwbNq0CZs2bcLLL7+MTp06ISkpqbFrJKq11we1g53SDOfSNNh68obU5RARkYGqVwB644030KZNG6SmpiI+Ph7x8fG4du0a/P398cYbbzR2jUS15mhjgekD2wAAFv9xCUWl5RJXREREhqheASg2NhaLFi2Co6OjbpmTkxM++ugjxMbGNlpxRPXxal9/eKgscSOnEN/HXZW6HCIiMkD1CkBKpRK5ubmVlufl5cHCwqLBRRE1hKX5n4/IWLY7ETkFJRJXREREhqZeAejZZ5/F1KlTcfjwYQghIITAoUOHMG3aNIwYMaKxaySqs+e7e6Ojux00RWVYvoePyCAiIn31CkBLly5FmzZtEBwcDEtLS1haWqJPnz5o27YtlixZ0sglEtWdQi7TPSJjzcEUpN4ukLgiIiIyJPW6Dd7BwQFbt25FYmKi7jb4gIAAtG3btlGLI2qIJ9u7oE8bJxxMysZnOy/h8zFdpS6JiIgMRK0nQqzLU94/++yzehdkCDgRYstx+roaw5ftBwD8+no/dPZSSVwRERE1lSaZCPHEiRO1aieTceI5MhyPeavwXFdPbE24icjfz2PtpCD+N0pERLUPQHv27GnKOoiazJwhHfD76XQcSMxGzKUsDOzgKnVJREQksXoNgiYyJj6O1hjfxxcA8I/NZ5BbVCpxRUREJDUGIDIJM0Paw8fRCjdyCvGvbeelLoeIiCTGAEQmwUZphk9eDAQArDuaij0XMyWuiIiIpMQARCajd2snTOzrBwB4d+MpqAt4KYyIyFQxAJFJeTu0I/ydbZChKcZ7/zsrdTlERCQRBiAyKVYWCiweFQi5DNh04gZ2nE2XuiQiIpIAAxCZnB6+rTD1iTYAgP/bfBq38/mwVCIiU8MARCbprafaob2bLW7llWDe1jNSl0NERM2MAYhMktJMgU9HdYVCLsO2U2n49dRNqUsiIqJmxABEJusxbxUiBlY8wHfeljPIyi2WuCIiImouDEBk0mYMbItHPexxp6AU/7f5NGr5bGAiIjJyDEBk0izM5PhsTCDMFTLsPJeBzSduSF0SERE1AwYgMnkd3e0xM6Q9AGDBL2eRri6SuCIiImpqkgagvXv3Yvjw4fD09IRMJsOWLVtqbL9p0yY89dRTcHFxgb29PYKDg/HHH3/otVm4cCFkMpneq2PHjk14FNQSvPZEawT6OCC3qAzvbDzFS2FERC2cpAEoPz8fgYGBWL58ea3a7927F0899RR+++03HD9+HAMHDsTw4cNx4sQJvXadOnVCWlqa7rV///6mKJ9aEDOFHJ+O6gILMzliL2Vh3dFUqUsiIqImZCblhw8bNgzDhg2rdfslS5bovf/www+xdetW/O9//0O3bt10y83MzODu7t5YZZKJaOtqh78N6YB//XYeH/x6Dv3aOsPH0VrqsoiIqAkY9RggrVaL3NxcODo66i2/fPkyPD090bp1a7z00ku4du1ajfspLi6GRqPRe5FperWfP3r6tkJ+STne/vkUtFpeCiMiaomMOgAtXrwYeXl5GD16tG5ZUFAQoqKisH37dqxYsQLJycno378/cnNzq91PZGQkVCqV7uXj49Mc5ZMBUshlWDwqEFbmCsRdycZ/DqVIXRIRETUBmTCQ0Z4ymQybN29GWFhYrdr/8MMPmDJlCrZu3YqQkJBq2+Xk5MDX1xefffYZJk2aVGWb4uJiFBf/OQmeRqOBj48P1Go17O3t63Qc1DKsOXgVC345CytzBX5/sz/8nG2kLomIiB5Co9FApVLV6ve3UfYArVu3DpMnT8aGDRtqDD8A4ODggPbt2yMxMbHaNkqlEvb29novMm2v9PZFcGsnFJaWY85PJ1HOS2FERC2K0QWgH3/8ERMnTsSPP/6IZ5555qHt8/LykJSUBA8Pj2aojloKuVyGRS92gY2FAsdS7mDV/mSpSyIiokYkaQDKy8tDQkICEhISAADJyclISEjQDVqeO3cuwsPDde1/+OEHhIeH49NPP0VQUBDS09ORnp4OtVqtazNnzhzExsbi6tWrOHjwIEaOHAmFQoFx48Y167GR8fNxtMY/nn0UAPDJjotIzKx+HBkRERkXSQPQsWPH0K1bN90t7LNmzUK3bt0wf/58AEBaWpreHVxff/01ysrKEBERAQ8PD93rzTff1LW5fv06xo0bhw4dOmD06NFwcnLCoUOH4OLi0rwHRy3C2F4+eLK9C0rKtJi94STKyrVSl0RERI3AYAZBG5K6DKKili9dXYQhn8dCU1SGv4V20D1BnoiIDEuLHwRN1JzcVZZYOKITAGDJrks4n8Z5ooiIjB0DEFEtjOzmhacedUNpucDsDSdRUsZLYURExowBiKgWZDIZPhz5GFpZm+NcmgbL9lQ/rQIRERk+BiCiWnKxU+L9sM4AgOV7EnH6uvohWxARkaFiACKqg2e7eOKZLh4o1wrM/ikBxWXlUpdERET1wABEVEfvP9cZzrYWuJSRh893Xpa6HCIiqgcGIKI6crSxwIcjHwMAfL03CfHX7khcERER1RUDEFE9DOnkjue7eUErgDkbTqKwhJfCiIiMCQMQUT0tGN4JbvZKXLmVj0/+uCh1OUREVAcMQET1pLI2x8cvdAEArD6YjMNXsiWuiIiIaosBiKgBBnRwxdhePhACmPPzSWiKSqUuiYiIaoEBiKiB/v5MALwcrJB6uxDT/nOct8YTERkBBiCiBrKzNMdXr/SArdIMB5OyMWvDSWi1fMYwEZEhYwAiagSdvVRY+XIPmCtk2HYqDf/89RyEYAgiIjJUDEBEjaRfO2d8OrorACDq4FWsjL0ibUFERFQtBiCiRjQi0BPznn0UAPDx9gv4+fh1iSsiIqKqMAARNbJJ/fzx2hOtAQDvbDyFPRczJa6IiIgexABE1ATeGdoRz3fzQrlW4K9r45GQmiN1SUREdB8GIKImIJfL8PGLXfBEexcUlpbj1aijuJKVJ3VZRER0FwMQURMxV8ix4qXu6OKtwu38EoSvOoJMTZHUZRERERiAiJqUjdIMqyb0gp+TNa7fKcT41Uc5WzQRkQFgACJqYs62Snz/ahCcbS1wPk3D2aKJiAwAAxBRM3jEyRpREx+HjYWCs0UTERkABiCiZtLZS4WvXunJ2aKJiAwAAxBRM+Js0UREhoEBiKiZcbZoIiLpMQARSYCzRRMRSYsBiEgi7wztiJGcLZqISBIMQEQSkctlWMTZoomIJMEARCQhzhZNRCQNBiAiiXG2aCKi5scARGQAOFs0EVHzkjQA7d27F8OHD4enpydkMhm2bNny0G1iYmLQvXt3KJVKtG3bFlFRUZXaLF++HH5+frC0tERQUBCOHDnS+MUTNTLOFk1E1HwkDUD5+fkIDAzE8uXLa9U+OTkZzzzzDAYOHIiEhATMnDkTkydPxh9//KFrs379esyaNQsLFixAfHw8AgMDERoaisxM3mZMho+zRRMRNQ+ZMJD/u8pkMmzevBlhYWHVtnnnnXewbds2nDlzRrds7NixyMnJwfbt2wEAQUFB6NWrF5YtWwYA0Gq18PHxweuvv4533323VrVoNBqoVCqo1WrY29vX/6CI6mlrwg28uS4BQMXt8tMHtJG2ICIiI1CX399GNQYoLi4OISEhestCQ0MRFxcHACgpKcHx48f12sjlcoSEhOjaVKW4uBgajUbvRSSl57p64R/PBADgbNFERE3BqAJQeno63Nzc9Ja5ublBo9GgsLAQt27dQnl5eZVt0tPTq91vZGQkVCqV7uXj49Mk9RPVxeT+rTlbNBFREzGqANRU5s6dC7VarXulpqZKXRIRAP3Zol/7z3FsZE8QEVGjMJO6gLpwd3dHRkaG3rKMjAzY29vDysoKCoUCCoWiyjbu7u7V7lepVEKpVDZJzUQNIZfL8PELXVBQUoY/zmZg9k8ncTEjF+8M7QiFXCZ1eURERsuoeoCCg4MRHR2tt2znzp0IDg4GAFhYWKBHjx56bbRaLaKjo3VtiIyNhZkcK17qgdcHtQUAfL33Cl6NOgp1ISdLJCKqL0kDUF5eHhISEpCQkACg4jb3hIQEXLt2DUDFpanw8HBd+2nTpuHKlSt4++23ceHCBXz55ZfYsGED3nrrLV2bWbNm4ZtvvsGaNWtw/vx5TJ8+Hfn5+Zg4cWKzHhtRY5LLZZg9pAO+GNcNluZyxF7KwsgvD/DZYURE9STpJbBjx45h4MCBuvezZs0CAIwfPx5RUVFIS0vThSEA8Pf3x7Zt2/DWW2/h3//+N7y9vfHtt98iNDRU12bMmDHIysrC/PnzkZ6ejq5du2L79u2VBkYTGaPhgZ7wd7bBlO+P4UpWPp5bfgDL/tIdT7Z3kbo0IiKjYjDzABkSzgNEhi4ztwjT/nMc8ddyIJcB//d0ACb184dMxnFBRGS6Wuw8QERUwdXOEj9O7Y0Xe3hDK4APtp3H334+xeeHERHVEgMQkZFSminwyYtdMO/ZRyGXAT8fv45xXx9CZm6R1KURERk8BiAiIyaTyTCpnz+iJj4Oe0szxF/LwXPLDuD0dbXUpRERGTQGIKIW4In2LtgS0RetXWyQpi7CiysP4peTN6Uui4jIYDEAEbUQrV1ssSWiLwZ0cEFxmRZv/HgCn/xxAVot73MgInoQAxBRC2JvaY7vxvfSPUNs+Z4kvLb2OPKKyySujIjIsDAAEbUwCrkMc58OwGejA2FhJsfOcxl4/ssDuJZdIHVpREQGgwGIqIV6vrs31k/tDVc7JS5l5GHE8v04mHRL6rKIiAwCAxBRC9btkVb4ZUY/dPFWIaegFK98dwT/ibsqdVlERJJjACJq4dxVltjwWjCe6+qJcq3AvK1n8ffNp1FSppW6NCIiyTAAEZkAS3MFlozpineGdoRMBvz38DW88t1hZOcVS10aEZEkGICITIRMJsP0AW3wbXhP2CrNcDj5Np5bfgDn0zRSl0ZE1OwYgIhMzOAAN2z+ax/4Olnj+p1CvLDiILafSZe6LCKiZsUARGSC2rnZYWtEX/Rt64SCknJMW3sckb+dR0EJ5wsiItPAAERkohysLRA18XFM6OMHAPhq7xU89dleRJ/PkLYwIqJmwABEZMLMFXIsHNEJ34T3hJeDFW7kFGLSmmOY+v0x3MgplLo8IqImwwBERHjqUTfsnPUEXnuyNczkMuw4l4GnPovF13uTUFrO2+WJqOVhACIiAIC1hRnmDgvAtjf6o5dfKxSUlOPD3y5g+Bf7cTzlttTlERE1KgYgItLTwd0O66cGY9ELXeBgbY4L6bl4YUUc3t14CnfyS6Quj4ioUTAAEVElcrkMo3v5YPfsARjd0xsAsO5oKgZ/Foufj1+HEELiComIGkYm+H+ySjQaDVQqFdRqNezt7aUuh0hyR6/ext83n8aljDwAwOP+jvhXWGe0c7OTuDIioj/V5fc3e4CI6KF6+Tli2xv98e6wjrAyV+BI8m0M+/c+fLz9AgpLyqUuj4iozhiAiKhWzBVyTHuyDXbOegIhAW4o0wqsiEnCU5/HYvcFzh1ERMaFAYiI6sS7lTW+Hd8TX7/SA54qS1y/U4hXo47htf8cw03OHURERoIBiIjqZUgnd+yc9SRee6I1FHIZ/jibgZDPYvHN3iucO4iIDB4HQVeBg6CJ6uZCugb/2HwGx1LuAAA6utvhXyMfQw/fVhJXRkSmhIOgiahZdXS3x4bXgvHxC4/dN3fQQczddAo5BZw7iIgMDwMQETUKuVyGMb0ewe7ZAzCqR8XcQT8eScWgTyvmDtJq2dlMRIaDl8CqwEtgRA13JPk2/rHlz7mD2rnaYvqANhge6AlzBf/tRUSNry6/vxmAqsAARNQ4Ssu1+HZfMr6MSURuURkAwMvBCq892Rqje/rA0lwhcYVE1JIwADUQAxBR49IUlWLtoRSs2p+MW3kVY4KcbS3waj9/vNzbF/aW5hJXSEQtAQNQAzEAETWNotJy/HQsFStjr+DG3TmD7JRmeCXYF6/284ezrVLiConImBndXWDLly+Hn58fLC0tERQUhCNHjlTbdsCAAZDJZJVezzzzjK7NhAkTKq0fOnRocxwKEdXA0lyBV4L9EPO3AfhsdCDaudoit7gMX8Ykoe9HuzF/6xlcv1MgdZlEZAIk7wFav349wsPDsXLlSgQFBWHJkiX46aefcPHiRbi6ulZqf/v2bZSU/HlbbXZ2NgIDA/Htt99iwoQJACoCUEZGBlavXq1rp1Qq0apV7eYkYQ8QUfPQagV2nc/A8pgknEzNAQCYyWUY0dUT059sw4etElGdGNUlsKCgIPTq1QvLli0DAGi1Wvj4+OD111/Hu++++9DtlyxZgvnz5yMtLQ02NjYAKgJQTk4OtmzZUq+aGICImpcQAnFJ2fgyJgn7E2/plg951A1/HdgWXX0cpCuOiIyG0VwCKykpwfHjxxESEqJbJpfLERISgri4uFrt47vvvsPYsWN14eeemJgYuLq6okOHDpg+fTqys7Or3UdxcTE0Go3ei4iaj0wmQ5+2zlg7OQhbI/piaCd3yGTAjnMZCFt+AH/55hD2X74FDlkkosYiaQC6desWysvL4ebmprfczc0N6enpD93+yJEjOHPmDCZPnqy3fOjQofj+++8RHR2Njz/+GLGxsRg2bBjKy8ur3E9kZCRUKpXu5ePjU/+DIqIGCfRxwMpXemDnW0/gxR7eMJPLcDApGy9/dxhhyw9g+5l0TqpIRA0m6SWwmzdvwsvLCwcPHkRwcLBu+dtvv43Y2FgcPny4xu1fe+01xMXF4dSpUzW2u3LlCtq0aYNdu3Zh8ODBldYXFxejuLhY916j0cDHx4eXwIgMwI2cQnyz9wrWHb2GotKKh6y2dbXFtCfb4LmunFSRiP5kNJfAnJ2doVAokJGRobc8IyMD7u7uNW6bn5+PdevWYdKkSQ/9nNatW8PZ2RmJiYlVrlcqlbC3t9d7EZFh8HKwwsIRnbD/nUGYMbAt7CzNkJiZhzk/ncSAT2IQdSAZhSVV9+4SEVVH0gBkYWGBHj16IDo6WrdMq9UiOjpar0eoKj/99BOKi4vx8ssvP/Rzrl+/juzsbHh4eDS4ZiKShrOtEnNCO+Dgu4PwztCOcLZV4kZOIRb+7xyCP4rGwl/O4swNNccJEVGtSH4X2Pr16zF+/Hh89dVXePzxx7FkyRJs2LABFy5cgJubG8LDw+Hl5YXIyEi97fr37w8vLy+sW7dOb3leXh7ee+89vPDCC3B3d0dSUhLefvtt5Obm4vTp01AqHz7RGu8CIzJ8RaXl+On4dXwVm4Trdwp1yzu62+HFHt54rqsXXOw4sSKRKanL72+zZqqpWmPGjEFWVhbmz5+P9PR0dO3aFdu3b9cNjL527Rrkcv2OqosXL2L//v3YsWNHpf0pFAqcOnUKa9asQU5ODjw9PTFkyBC8//77tQo/RGQcLM0VeKW3L8b18sH+xFv4+fh17DiXgQvpufhg23lE/n4BA9q74MUe3hgU4AqlGZ87RkR/krwHyBCxB4jIOKkLSvG/UzexMf46TlzL0S13sDbHc4GeeLGHDzp72UMmk0lXJBE1GaOaCNEQMQARGb/EzDxsjL+OTfHXkaH58y7P9m62eLGHN8K6ecHVzlLCComosTEANRADEFHLUa4VOHD3EtkfZ9NRXFZxK71CLsOTdy+RDeYlMqIWgQGogRiAiFomdWEptp1Kw8/HUxF/3yUylZU5RgR64sUe3ujireIlMiIjxQDUQAxARC1fUlYeNsVfx6b4G0hTF+mWt3OtuEQ2spsXXO15iYzImDAANRADEJHpKNcKHEyquES2/cyfl8jkMuCJu5fIQgLcYGnOS2REho4BqIEYgIhMk6aoFL+dSsPPx6/jWMod3XJrCwX6t3NGSIAbBnV0hZMtp9QgMkQMQA3EAEREybfydZfIbuT8OdGiTAZ0f6QVQgLcEBLgirauthwzRGQgGIAaiAGIiO4RQuDsTQ12nstA9IUMnLmh0Vvv62SNkAA3DA5wRS8/Rz6clUhCDEANxABERNW5mVOI6AuZiD6fgYOJ2Sgp1+rW2VuaYWBHVwwOcMOT7V2gsjKXsFIi08MA1EAMQERUG/nFZdh3+RZ2nc/A7guZuJ1foltnJpchqLUjBnd0Q0iAGx5xspawUiLTwADUQAxARFRX5VqBhNQ72HkuE7vOZyAxM09vfQc3OwwOcEXIo27o6u0AuZzjhogaGwNQAzEAEVFDXb2Vj13nMxB9PhNHrt5GufbP/9U621pgUEdXhAS4oV87Z1hbSP5caqIWgQGogRiAiKgxqQtKEXMpE7vOZyLmQiZyi8t065RmcvT0a4Xe/k7o3cYJgd4OsDDjQGqi+mAAaiAGICJqKiVlWhy9ehu7zmdg1/kMpN4u1FtvaS5HT19H9G7tiN6tndCFgYio1hiAGogBiIiagxACSVl5iLtyG4eSsnHoSjay7xtIDTAQEdUFA1ADMQARkRSEEEjMzMOhK9k4dOV2lYHIylxRccmstRN6t3bEY14MRET3MAA1EAMQERkCIQQu6wJRRSi6XWMgckIXbxUnYySTxQDUQAxARGSIahOIrC0U6OHLQESmiQGogRiAiMgYaLUPBqJs3Cko1WtjZa7AY14qBPqoEOjjgEBvB3i3suLzy6hFYgBqIAYgIjJGtQlEAOBkY6ELQ4E+KgR6O6CVjYUEFRM1LgagBmIAIqKWQKutuMvs5HU1Tqbm4OT1HJxP06C0vPL/9n2drO8GIgd09VGhk6cKluYKCaomqj8GoAZiACKilqqotBzn0zQ4dTcUJVzPwZWs/ErtFHIZOrrb3e0pqrh81s7VDgo+woMMGANQAzEAEZEpUReW4vR1NU5ez0FCasUrK7e4UjtrCwU6e6nQ9b7LZ14OHE9EhoMBqIEYgIjIlAkhkK4pqughSq3oKTp1PQf5JeWV2jraWKCjux06utsjwMMOAR72aOtqy8tnJAkGoAZiACIi0leuFbiSlYeEu2OJTqaqcT5NgzJt5V8hCrkM/s426OheEYg6utuho4c9PFWW7C2iJsUA1EAMQERED1dUWo7LGXk4n67BhbRcXEjX4Hyapso7zwDAztIMAXd7ijreDUYd3O1gbWHWzJVTS8UA1EAMQERE9SOEQGZuMc6naXAhPRcX7v6ZmJlXZW+RTAb4Olqjo7s9Onr8eSnNp5U15BxwTXXEANRADEBERI2rpEyLpKw8XLjbW3TubjCqarA1ANhYKNDB3Q7tXO3QxtUGbVxs0cbFFt6trGDGma2pGgxADcQARETUPG7lFeNieu6fPUbpGlzKyENJmbbK9hYKOfycrdHa2VYvGLV2sYGdpXkzV0+GhgGogRiAiIikU1auRfKtfJy/e+nsSlYekrLycSUrD8XVBCMAcLNXVgpGbVxt4WFvyctpJoIBqIEYgIiIDI9WK3AjpxBJ9wWie3+v7lIaUPE8NH9nG7RxtUUblz/Dkb+zDawseLt+S2J0AWj58uX45JNPkJ6ejsDAQHzxxRd4/PHHq2wbFRWFiRMn6i1TKpUoKirSvRdCYMGCBfjmm2+Qk5ODvn37YsWKFWjXrl2t6mEAIiIyLurCUlzJysOVrPy7oagiGKVk51f56I973O0t8YiTNXwdrfGIo3XF351s4OtoDQdrc962b2Tq8vtb8nsP169fj1mzZmHlypUICgrCkiVLEBoaiosXL8LV1bXKbezt7XHx4kXd+wf/A120aBGWLl2KNWvWwN/fH/PmzUNoaCjOnTsHS0vLJj0eIiJqfiorc3R7pBW6PdJKb3lpuRaptwsqBaPEzDyoC0uRrilCuqYIR5JvV9qnnaUZfJ3uBiNHG/jeC0pO1vBQWfGxIEZO8h6goKAg9OrVC8uWLQMAaLVa+Pj44PXXX8e7775bqX1UVBRmzpyJnJycKvcnhICnpydmz56NOXPmAADUajXc3NwQFRWFsWPHPrQm9gAREbVsQgjcKShFSnY+rt0uQEp2xSv1dgFSbucjQ1P9JTUAMFfI4N2qIhzdC0m+ThUhyaeVNS+tScRoeoBKSkpw/PhxzJ07V7dMLpcjJCQEcXFx1W6Xl5cHX19faLVadO/eHR9++CE6deoEAEhOTkZ6ejpCQkJ07VUqFYKCghAXF1erAERERC2bTCaDo40FHG0sKvUaAUBhSTlS79wLRvl3g1EBrmUXIPVOAUrLBZJv5SP5VuUHyQKAq50Sjzhaw6uVFbwcrODVygqeDlbwvvt3Tv4oPUnPwK1bt1BeXg43Nze95W5ubrhw4UKV23To0AGrVq1Cly5doFarsXjxYvTp0wdnz56Ft7c30tPTdft4cJ/31j2ouLgYxcV/pn2NRtOQwyIiIiNnZaFAezc7tHezq7SuXCuQpi7EtbuBKEX3Zz5SsguQW1SGzNxiZOYW41jKnSr338raHJ4Of4YjLwf9oORkY8HxR03M6CJocHAwgoODde/79OmDgIAAfPXVV3j//ffrtc/IyEi89957jVUiERG1YAp5xeUv71bW6NNGf50QAurCUqRkF+Da7QLczCnEjZxC3Lhz98+cQuQWleFOQSnuFJTi7M2q/8FtaS7/MyA9EI68HKzgrrKEOSeEbBBJA5CzszMUCgUyMjL0lmdkZMDd3b1W+zA3N0e3bt2QmJgIALrtMjIy4OHhobfPrl27VrmPuXPnYtasWbr3Go0GPj4+dTkUIiIiyGQyOFhbwMHaAoE+DlW20RSVVgSjO5XD0Y07hcjMLUZRqRZXsvJxJavqS2xyGeBqZwk3lSU87C3hrrr7sreEm70lPO6+tzTnWKTqSBqALCws0KNHD0RHRyMsLAxAxSDo6OhozJgxo1b7KC8vx+nTp/H0008DAPz9/eHu7o7o6Ghd4NFoNDh8+DCmT59e5T6USiWUSmWDj4eIiOhh7C3NYe9ujo7uVQ/SLS4rR7q6qFIwuqm++2dOEUrKtbo72E7W8FkqK3N4qP4MRW4PhCV3e0uTvd1f8ktgs2bNwvjx49GzZ088/vjjWLJkCfLz83Vz/YSHh8PLywuRkZEAgH/+85/o3bs32rZti5ycHHzyySdISUnB5MmTAVSk75kzZ+KDDz5Au3btdLfBe3p66kIWERGRoVKaKe7eUWZT5XqtVuBWXjHS1EVIUxch424QSlfffd39e2FpOdSFpVAXluJCem4NnyeH+30h6V4vkpu9JVztlXCxVcLVXtniBm5LfjRjxoxBVlYW5s+fj/T0dHTt2hXbt2/XDWK+du0a5PI/r3PeuXMHU6ZMQXp6Olq1aoUePXrg4MGDePTRR3Vt3n77beTn52Pq1KnIyclBv379sH37ds4BRERERk8ul8HV3hKu9pYIrGa0hhACmqIyXSDKuBuWKsJRIdI1xcjQFOF2fgmKy7S6aQBqYmOhgKu9JVxslXCxV8LVTgkXOyVc7Szv/lnx3tHawigePSL5PECGiPMAERGRKSgqLUemphjpmiKkqQsrepPUxUjXFCJTU4ysvGJkaopRWFpe630q5DI421rA1c7yvpBU8afLA2GpsccoGc08QERERCQdS3MFHnGqmN26OkII5JeUI1NThMzcYmTdvcW/4s8iZN39e1ZuMbLzS1CuFcjQFD90MskJffywcESnxj6kWmMAIiIiomrJZDLYKs1g62KL1i62NbYtLdciO69EF4wyc4vv9iQV6fUoZeUVw8VO2puPGICIiIioUZgr5Lq7zGoihEC5VtoROAxARERE1KxkMhnMFNIOlOY0kkRERGRyGICIiIjI5DAAERERkclhACIiIiKTwwBEREREJocBiIiIiEwOAxARERGZHAYgIiIiMjkMQERERGRyGICIiIjI5DAAERERkclhACIiIiKTwwBEREREJodPg6+CEAIAoNFoJK6EiIiIauve7+17v8drwgBUhdzcXACAj4+PxJUQERFRXeXm5kKlUtXYRiZqE5NMjFarxc2bN2FnZweZTNao+9ZoNPDx8UFqairs7e0bdd+GhsfacpnS8fJYWy5TOl5TOVYhBHJzc+Hp6Qm5vOZRPuwBqoJcLoe3t3eTfoa9vX2L/o/wfjzWlsuUjpfH2nKZ0vGawrE+rOfnHg6CJiIiIpPDAEREREQmhwGomSmVSixYsABKpVLqUpocj7XlMqXj5bG2XKZ0vKZ0rLXFQdBERERkctgDRERERCaHAYiIiIhMDgMQERERmRwGICIiIjI5DEBNYPny5fDz84OlpSWCgoJw5MiRGtv/9NNP6NixIywtLfHYY4/ht99+a6ZK6y8yMhK9evWCnZ0dXF1dERYWhosXL9a4TVRUFGQymd7L0tKymSquv4ULF1aqu2PHjjVuY4zn9B4/P79KxyuTyRAREVFle2M6r3v37sXw4cPh6ekJmUyGLVu26K0XQmD+/Pnw8PCAlZUVQkJCcPny5Yfut64/882hpmMtLS3FO++8g8ceeww2Njbw9PREeHg4bt68WeM+6/Oz0Fwedm4nTJhQqfahQ4c+dL/Gdm4BVPnzK5PJ8Mknn1S7T0M+t02FAaiRrV+/HrNmzcKCBQsQHx+PwMBAhIaGIjMzs8r2Bw8exLhx4zBp0iScOHECYWFhCAsLw5kzZ5q58rqJjY1FREQEDh06hJ07d6K0tBRDhgxBfn5+jdvZ29sjLS1N90pJSWmmihumU6dOenXv37+/2rbGek7vOXr0qN6x7ty5EwAwatSoarcxlvOan5+PwMBALF++vMr1ixYtwtKlS7Fy5UocPnwYNjY2CA0NRVFRUbX7rOvPfHOp6VgLCgoQHx+PefPmIT4+Hps2bcLFixcxYsSIh+63Lj8Lzelh5xYAhg4dqlf7jz/+WOM+jfHcAtA7xrS0NKxatQoymQwvvPBCjfs11HPbZAQ1qscff1xERETo3peXlwtPT08RGRlZZfvRo0eLZ555Rm9ZUFCQeO2115q0zsaWmZkpAIjY2Nhq26xevVqoVKrmK6qRLFiwQAQGBta6fUs5p/e8+eabok2bNkKr1Va53ljPKwCxefNm3XutVivc3d3FJ598oluWk5MjlEql+PHHH6vdT11/5qXw4LFW5ciRIwKASElJqbZNXX8WpFLV8Y4fP14899xzddpPSzm3zz33nBg0aFCNbYzl3DYm9gA1opKSEhw/fhwhISG6ZXK5HCEhIYiLi6tym7i4OL32ABAaGlpte0OlVqsBAI6OjjW2y8vLg6+vL3x8fPDcc8/h7NmzzVFeg12+fBmenp5o3bo1XnrpJVy7dq3ati3lnAIV/02vXbsWr776ao0PBjbW83q/5ORkpKen6507lUqFoKCgas9dfX7mDZVarYZMJoODg0ON7erys2BoYmJi4Orqig4dOmD69OnIzs6utm1LObcZGRnYtm0bJk2a9NC2xnxu64MBqBHdunUL5eXlcHNz01vu5uaG9PT0KrdJT0+vU3tDpNVqMXPmTPTt2xedO3eutl2HDh2watUqbN26FWvXroVWq0WfPn1w/fr1Zqy27oKCghAVFYXt27djxYoVSE5ORv/+/ZGbm1tl+5ZwTu/ZsmULcnJyMGHChGrbGOt5fdC981OXc1efn3lDVFRUhHfeeQfjxo2r8UGZdf1ZMCRDhw7F999/j+joaHz88ceIjY3FsGHDUF5eXmX7lnJu16xZAzs7Ozz//PM1tjPmc1tffBo8NVhERATOnDnz0OvFwcHBCA4O1r3v06cPAgIC8NVXX+H9999v6jLrbdiwYbq/d+nSBUFBQfD19cWGDRtq9a8qY/bdd99h2LBh8PT0rLaNsZ5XqlBaWorRo0dDCIEVK1bU2NaYfxbGjh2r+/tjjz2GLl26oE2bNoiJicHgwYMlrKxprVq1Ci+99NJDb0ww5nNbX+wBakTOzs5QKBTIyMjQW56RkQF3d/cqt3F3d69Te0MzY8YM/Prrr9izZw+8vb3rtK25uTm6deuGxMTEJqquaTg4OKB9+/bV1m3s5/SelJQU7Nq1C5MnT67TdsZ6Xu+dn7qcu/r8zBuSe+EnJSUFO3furLH3pyoP+1kwZK1bt4azs3O1tRv7uQWAffv24eLFi3X+GQaM+9zWFgNQI7KwsECPHj0QHR2tW6bVahEdHa33L+T7BQcH67UHgJ07d1bb3lAIITBjxgxs3rwZu3fvhr+/f533UV5ejtOnT8PDw6MJKmw6eXl5SEpKqrZuYz2nD1q9ejVcXV3xzDPP1Gk7Yz2v/v7+cHd31zt3Go0Ghw8frvbc1edn3lDcCz+XL1/Grl274OTkVOd9POxnwZBdv34d2dnZ1dZuzOf2nu+++w49evRAYGBgnbc15nNba1KPwm5p1q1bJ5RKpYiKihLnzp0TU6dOFQ4ODiI9PV0IIcQrr7wi3n33XV37AwcOCDMzM7F48WJx/vx5sWDBAmFubi5Onz4t1SHUyvTp04VKpRIxMTEiLS1N9yooKNC1efBY33vvPfHHH3+IpKQkcfz4cTF27FhhaWkpzp49K8Uh1Nrs2bNFTEyMSE5OFgcOHBAhISHC2dlZZGZmCiFazjm9X3l5uXjkkUfEO++8U2mdMZ/X3NxcceLECXHixAkBQHz22WfixIkTujufPvroI+Hg4CC2bt0qTp06JZ577jnh7+8vCgsLdfsYNGiQ+OKLL3TvH/YzL5WajrWkpESMGDFCeHt7i4SEBL2f4eLiYt0+HjzWh/0sSKmm483NzRVz5swRcXFxIjk5WezatUt0795dtGvXThQVFen20RLO7T1qtVpYW1uLFStWVLkPYzq3TYUBqAl88cUX4pFHHhEWFhbi8ccfF4cOHdKte/LJJ8X48eP12m/YsEG0b99eWFhYiE6dOolt27Y1c8V1B6DK1+rVq3VtHjzWmTNn6r4XNzc38fTTT4v4+PjmL76OxowZIzw8PISFhYXw8vISY8aMEYmJibr1LeWc3u+PP/4QAMTFixcrrTPm87pnz54q/7u9dzxarVbMmzdPuLm5CaVSKQYPHlzpO/D19RULFizQW1bTz7xUajrW5OTkan+G9+zZo9vHg8f6sJ8FKdV0vAUFBWLIkCHCxcVFmJubC19fXzFlypRKQaYlnNt7vvrqK2FlZSVycnKq3IcxndumIhNCiCbtYiIiIiIyMBwDRERERCaHAYiIiIhMDgMQERERmRwGICIiIjI5DEBERERkchiAiIiIyOQwABEREZHJYQAiMgEDBgzAzJkzpS6jEplMhi1btkhdBl555RV8+OGHUpfRrFauXInhw4dLXQaRZDgRIpEJuH37NszNzWFnZwcA8PPzw8yZM5stFC1cuBBbtmxBQkKC3vL09HS0atUKSqWyWeqoysmTJzFo0CCkpKTA1ta22T8/KioKM2fORE5OTrN+bklJCfz9/bFu3Tr079+/WT+byBCwB4jIBDg6OurCT2MqKSlp0Pbu7u6Shh8A+OKLLzBq1KgmDz8N/a4am4WFBf7yl79g6dKlUpdCJAkGICITcP8lsAEDBiAlJQVvvfUWZDIZZDKZrt3+/fvRv39/WFlZwcfHB2+88Qby8/N16/38/PD+++8jPDwc9vb2mDp1KgDgnXfeQfv27WFtbY3WrVtj3rx5KC0tBVDRw/Hee+/h5MmTus+LiooCUPkS2OnTpzFo0CBYWVnByckJU6dORV5enm79hAkTEBYWhsWLF8PDwwNOTk6IiIjQfRYAfPnll2jXrh0sLS3h5uaGF198sdrvpby8HD///HOlS0H3jnPcuHGwsbGBl5cXli9frtcmJycHkydPhouLC+zt7TFo0CCcPHlSt37hwoXo2rUrvv32W/j7+8PS0rLS58fExGDixIlQq9W672bhwoUAgOLiYsyZMwdeXl6wsbFBUFAQYmJidNtGRUXBwcEBf/zxBwICAmBra4uhQ4ciLS1Nb/+PP/44bGxs4ODggL59+yIlJUW3fvjw4fjll19QWFhY7XdE1GJJ+ygyImoOTz75pHjzzTeFEEJkZ2cLb29v8c9//lP3BHAhhEhMTBQ2Njbi888/F5cuXRIHDhwQ3bp1ExMmTNDtx9fXV9jb24vFixeLxMRE3cMS33//fXHgwAGRnJwsfvnlF+Hm5iY+/vhjIYQQBQUFYvbs2aJTp066zysoKBBCVDxUd/PmzUIIIfLy8oSHh4d4/vnnxenTp0V0dLTw9/fXe8Dj+PHjhb29vZg2bZo4f/68+N///iesra3F119/LYQQ4ujRo0KhUIgffvhBXL16VcTHx4t///vf1X4v8fHxAkCVD8W0s7MTkZGR4uLFi2Lp0qVCoVCIHTt26NqEhISI4cOHi6NHj4pLly6J2bNnCycnJ5GdnS2EEGLBggXCxsZGDB06VMTHx4uTJ09W+vzi4mKxZMkSYW9vr/tucnNzhRBCTJ48WfTp00fs3btXJCYmik8++UQolUpx6dIlIYQQq1evFubm5iIkJEQcPXpUHD9+XAQEBIi//OUvQgghSktLhUqlEnPmzBGJiYni3LlzIioqSu+J4fn5+UIul+s9AJXIVDAAEZmA+wOQEBW/4D///HO9NpMmTRJTp07VW7Zv3z4hl8tFYWGhbruwsLCHft4nn3wievTooXu/YMECERgYWKnd/QHo66+/Fq1atRJ5eXm69du2bRNyuVwXUMaPHy98fX1FWVmZrs2oUaPEmDFjhBBCbNy4Udjb2wuNRvPQGoUQYvPmzUKhUAitVqu33NfXVwwdOlRv2ZgxY8SwYcOEEBXfi729vSgqKtJr06ZNG/HVV1/pjtnc3FxkZmbWWMPq1auFSqXSW5aSkiIUCoW4ceOG3vLBgweLuXPn6rYDoPfE7uXLlws3NzchREXQBSBiYmJq/PxWrVqJqKioGtsQtURmknY/EZHBOHnyJE6dOoX//ve/umVCCGi1WiQnJyMgIAAA0LNnz0rbrl+/HkuXLkVSUhLy8vJQVlYGe3v7On3++fPnERgYCBsbG92yvn37QqvV4uLFi3BzcwMAdOrUCQqFQtfGw8MDp0+fBgA89dRT8PX1RevWrTF06FAMHToUI0eOhLW1dZWfWVhYCKVSqXcZ8J7g4OBK75csWQKg4rvKy8uDk5NTpf0lJSXp3vv6+sLFxaUO30KF06dPo7y8HO3bt9dbXlxcrPeZ1tbWaNOmje69h4cHMjMzAVSM+5owYQJCQ0Px1FNPISQkBKNHj4aHh4fePq2srFBQUFDnGomMHQMQEQEA8vLy8Nprr+GNN96otO6RRx7R/f3+gAIAcXFxeOmll/Dee+8hNDQUKpUK69atw6efftokdZqbm+u9l8lk0Gq1AAA7OzvEx8cjJiYGO3bswPz587Fw4UIcPXoUDg4Olfbl7OyMgoIClJSUwMLCotY15OXlwcPDQ29Mzj33f86D31Vd9q9QKHD8+HG9sAdAb7B2Vd+FuO/G3tWrV+ONN97A9u3bsX79evzjH//Azp070bt3b12b27dv1yukERk7BiAiE2RhYYHy8nK9Zd27d8e5c+fQtm3bOu3r4MGD8PX1xd///nfdsvsH2lb3eQ8KCAhAVFQU8vPzdcHhwIEDkMvl6NChQ63rMTMzQ0hICEJCQrBgwQI4ODhg9+7deP755yu17dq1KwDg3Llzur/fc+jQoUrv7/WCde/eHenp6TAzM4Ofn1+ta6tKVd9Nt27dUF5ejszMzAbfot6tWzd069YNc+fORXBwMH744QddAEpKSkJRURG6devWoM8gMka8C4zIBPn5+WHv3r24ceMGbt26BaDiTq6DBw9ixowZSEhIwOXLl7F161bMmDGjxn21a9cO165dw7p165CUlISlS5di8+bNlT4vOTkZCQkJuHXrFoqLiyvt56WXXoKlpSXGjx+PM2fOYM+ePXj99dfxyiuv6C5/Pcyvv/6KpUuXIiEhASkpKfj++++h1WqrDVAuLi7o3r079u/fX2ndgQMHsGjRIly6dAnLly/HTz/9hDfffBMAEBISguDgYISFhWHHjh24evUqDh48iL///e84duxYrWq9x8/PD3l5eYiOjsatW7dQUFCA9u3b46WXXkJ4eDg2bdqE5ORkHDlyBJGRkdi2bVut9pucnIy5c+ciLi4OKSkp2LFjBy5fvqwLcQCwb98+tG7dWu8yGpGpYAAiMkH//Oc/cfXqVbRp00Z3+aNLly6IjY3FpUuX0L9/f3Tr1g3z58+Hp6dnjfsaMWIE3nrrLcyYMQNdu3bFwYMHMW/ePL02L7zwAoYOHYqBAwfCxcUFP/74Y6X9WFtb448//sDt27fRq1cvvPjiixg8eDCWLVtW6+NycHDApk2bMGjQIAQEBGDlypX48ccf0alTp2q3mTx5st64p3tmz56NY8eOoVu3bvjggw/w2WefITQ0FEDFpabffvsNTzzxBCZOnIj27dtj7NixSElJqXVYu6dPnz6YNm0axowZAxcXFyxatAhAxeWr8PBwzJ49Gx06dEBYWBiOHj2qdzmyJtbW1rhw4QJeeOEFtG/fHlOnTkVERARee+01XZsff/wRU6ZMqVO9RC0FZ4ImIpNWWFiIDh06YP369bqBz809U7YUzp49i0GDBuHSpUtQqVRSl0PU7NgDREQmzcrKCt9//73uUqCpSEtLw/fff8/wQyaLg6CJyOQNGDBA6hKaXUhIiNQlEEmKl8CIiIjI5PASGBEREZkcBiAiIiIyOQxAREREZHIYgIiIiMjkMAARERGRyWEAIiIiIpPDAEREREQmhwGIiIiITA4DEBEREZmc/wfY7B5CFK7BzgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1min 47s (started: 2022-09-25 04:13:31 +05:30)\n"
          ]
        }
      ],
      "source": [
        "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "loJCqG9rQjlk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 348 µs (started: 2022-09-25 04:15:19 +05:30)\n"
          ]
        }
      ],
      "source": [
        "def predict(X, y, parameters):\n",
        "    \n",
        "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_layer_forward(X, parameters)\n",
        "    \n",
        "    p = np.argmax(probas, axis = 0)\n",
        "    act = np.argmax(y, axis = 0)\n",
        "\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
        "        \n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G207-umJQjlk"
      },
      "source": [
        "Let's see the accuray we get on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3UnT_sbOQjlk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8774000000000002\n",
            "time: 41.1 ms (started: 2022-09-25 04:15:19 +05:30)\n"
          ]
        }
      ],
      "source": [
        "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEGGnj1XQjlk"
      },
      "source": [
        "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3xVFiw-UQjlk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8674000000000002\n",
            "time: 34.4 ms (started: 2022-09-25 04:15:19 +05:30)\n"
          ]
        }
      ],
      "source": [
        "pred_test = predict(test_set_x, test_set_y, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wit0bdqHQjll"
      },
      "source": [
        "It is ~87%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
        "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JWNfL6zhQjll"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x177789780>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 72 ms (started: 2022-09-25 04:15:20 +05:30)\n"
          ]
        }
      ],
      "source": [
        "index  = 3474\n",
        "k = test_set_x[:,index]\n",
        "k = k.reshape((28, 28))\n",
        "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
        "plt.imshow(k, cmap='gray')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment_solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
